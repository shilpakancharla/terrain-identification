{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "owned-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import statistics as st\n",
    "from scipy import stats\n",
    "from keras import backend as K\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install keras-rectified-adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bibliographic-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_data(X_file, y_file):\n",
    "    # Read in both CSV files\n",
    "    df_X = pd.read_csv(X_file)\n",
    "    df_y = pd.read_csv(y_file)\n",
    "    \n",
    "    extrapolated_labels = []\n",
    "    # Iterate through every item (row) of the y labels\n",
    "    for label in df_y.iterrows():\n",
    "        extrapolated_labels += [label[1][0]] * 4\n",
    "    \n",
    "    extrapolated_labels_df = pd.DataFrame(extrapolated_labels)\n",
    "    difference = df_X.shape[0] - extrapolated_labels_df.shape[0]\n",
    "    df_X = df_X.iloc[:-difference,:]\n",
    "    \n",
    "    return df_X, extrapolated_labels_df\n",
    "\n",
    "\"\"\"\n",
    "    Scale the values of X to make it robust to outliers.\n",
    "    \n",
    "    @param df: input dataframe\n",
    "    @param columns: columns to scale\n",
    "    @return scaled dataframe\n",
    "\"\"\"\n",
    "def scale_data(df, columns):\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(df[columns])\n",
    "    df.loc[:, columns] = scaler.transform(df[columns].to_numpy())\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "    Create time-series data from our X and y.\n",
    "\"\"\"\n",
    "def mode_labels(X, y, time_step, step_size):\n",
    "    X_values = []\n",
    "    y_values = []\n",
    "    for i in range(0, len(X) - time_step, step_size):\n",
    "        value = X.iloc[i:(i + time_step)].values\n",
    "        labels = y.iloc[i:(i + time_step)]\n",
    "        X_values.append(value)\n",
    "        y_values.append(stats.mode(labels)[0][0])\n",
    "    return np.array(X_values), np.array(y_values).reshape(-1, 1)\n",
    "\n",
    "def create_time_series_data(X_files, y_files, time_step, step_size):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    for i in range(len(y_files)):\n",
    "        X, y = extrapolate_data(X_files[i], y_files[i])\n",
    "        X = scale_data(X, list(X.columns.values))\n",
    "        X, y = mode_labels(X, y, time_step, step_size)\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    return np.concatenate(all_X), np.concatenate(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "simplified-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of training, validation, and test X_files\n",
    "X_files = ['TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_02__x.csv', \n",
    "           'TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_04__x.csv', \n",
    "           'TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_06__x.csv', \n",
    "           'TrainingData/subject_001_07__x.csv', 'TrainingData/subject_002_02__x.csv', \n",
    "           'TrainingData/subject_002_03__x.csv', 'TrainingData/subject_002_04__x.csv', \n",
    "           'TrainingData/subject_002_05__x.csv', 'TrainingData/subject_003_01__x.csv', \n",
    "           'TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_03__x.csv', \n",
    "           'TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_02__x.csv', \n",
    "           'TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_02__x.csv', \n",
    "           'TrainingData/subject_005_03__x.csv', 'TrainingData/subject_006_01__x.csv', \n",
    "           'TrainingData/subject_006_02__x.csv', 'TrainingData/subject_007_02__x.csv', \n",
    "           'TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_04__x.csv',\n",
    "           'TrainingData/subject_008_01__x.csv']\n",
    "\n",
    "val_X_files = ['TrainingData/subject_002_01__x.csv', 'TrainingData/subject_001_08__x.csv']\n",
    "test_X_files = ['TrainingData/subject_006_03__x.csv', 'TrainingData/subject_007_01__x.csv']\n",
    "\n",
    "# List of training, validation, and test y_files\n",
    "y_files = ['TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_02__y.csv', \n",
    "           'TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_04__y.csv', \n",
    "           'TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_06__y.csv', \n",
    "           'TrainingData/subject_001_07__y.csv', 'TrainingData/subject_002_02__y.csv',\n",
    "           'TrainingData/subject_002_03__y.csv', 'TrainingData/subject_002_04__y.csv', \n",
    "           'TrainingData/subject_002_05__y.csv', 'TrainingData/subject_003_01__y.csv', \n",
    "           'TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_03__y.csv', \n",
    "           'TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_02__y.csv', \n",
    "           'TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_02__y.csv', \n",
    "           'TrainingData/subject_005_03__y.csv', 'TrainingData/subject_006_01__y.csv', \n",
    "           'TrainingData/subject_006_02__y.csv', 'TrainingData/subject_007_02__y.csv', \n",
    "           'TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_04__y.csv',\n",
    "           'TrainingData/subject_008_01__y.csv']\n",
    "\n",
    "val_y_files = ['TrainingData/subject_002_01__y.csv', 'TrainingData/subject_001_08__y.csv']\n",
    "test_y_files = ['TrainingData/subject_006_03__y.csv', 'TrainingData/subject_007_01__y.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "violent-gathering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1126710, 30, 6) (1126710, 1)\n",
      "(122696, 30, 6) (122696, 1)\n",
      "(91260, 30, 6) (91260, 1)\n"
     ]
    }
   ],
   "source": [
    "training_X, training_y = create_time_series_data(X_files, y_files, 30, 1)\n",
    "val_X, val_y = create_time_series_data(val_X_files, val_y_files, 30, 1)\n",
    "test_X, test_y = create_time_series_data(test_X_files, test_y_files, 30, 1)\n",
    "\n",
    "print(training_X.shape, training_y.shape)\n",
    "print(val_X.shape, val_y.shape)\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "round-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data to .npy files so we do not have to generate them repeatedly\n",
    "np.save('processed_data/training_X.npy', training_X)\n",
    "np.save('processed_data/training_y.npy', training_y)\n",
    "np.save('processed_data/val_X.npy', val_X)\n",
    "np.save('processed_data/val_y.npy', val_y)\n",
    "np.save('processed_data/test_X.npy', test_X)\n",
    "np.save('processed_data/test_y.npy', test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "signed-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data back\n",
    "training_X = np.load('processed_data/training_X.npy')\n",
    "training_y = np.load('processed_data/training_y.npy')\n",
    "val_X = np.load('processed_data/val_X.npy')\n",
    "val_y = np.load('processed_data/val_y.npy')\n",
    "test_X = np.load('processed_data/test_X.npy')\n",
    "test_y = np.load('processed_data/test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powered-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We handle the data imbalance by assign higher weights to minority classes.\n",
    "\n",
    "    @param training_X: training X data\n",
    "    @param training_y: labels for training data\n",
    "    @return dictionary of labels as key and weights as values\n",
    "\"\"\"\n",
    "def get_label_weights(training_X, training_y):\n",
    "    label_weights = class_weight.compute_class_weight('balanced', np.unique(training_y), training_y.ravel())\n",
    "    label_weights = {i:label_weights[i] for i in range(len(label_weights))}\n",
    "    return label_weights\n",
    "\n",
    "\"\"\"\n",
    "    Perform one-hot encoding of the data to feed into our model.\n",
    "\n",
    "    @param labels: labels of the training data\n",
    "    @return one-hot encoded version of the labels\n",
    "\"\"\"\n",
    "def one_hot_encoding(labels):\n",
    "    encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "    encoder = encoder.fit(labels)\n",
    "    training_y_encoded = encoder.transform(labels)\n",
    "    return training_y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alpha-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3258715475333822, 1: 5.860711164745537, 2: 4.427916810764926, 3: 1.8697229376310969}\n",
      "(1126710, 4) (122696, 4) (91260, 4)\n"
     ]
    }
   ],
   "source": [
    "# Get training label weights\n",
    "label_weights = get_label_weights(training_X, training_y)\n",
    "print(label_weights)\n",
    "\n",
    "# Perform one-hot encoding on all labels\n",
    "training_y_encoded = one_hot_encoding(training_y)\n",
    "val_y_encoded = one_hot_encoding(val_y)\n",
    "test_y_encoded = one_hot_encoding(test_y)\n",
    "print(training_y_encoded.shape, val_y_encoded.shape, test_y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hollywood-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Calculate recall from predicted and actual values.\n",
    "\n",
    "    @param y_true: actual y values\n",
    "    @param y_pred: predicted y values\n",
    "\"\"\"\n",
    "def recall_measure(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\"\"\"\n",
    "    Calculate precision from predicted and actual values.\n",
    "\n",
    "    @param y_true: actual y values\n",
    "    @param y_pred: predicted y values\n",
    "\"\"\"\n",
    "def precision_measure(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\"\"\"\n",
    "    Calculate F1-score from predicted and actual values.\n",
    "\n",
    "    @param y_true: actual y values\n",
    "    @param y_pred: predicted y values\n",
    "\"\"\"\n",
    "def f1(y_true, y_pred):\n",
    "    precision = precision_measure(y_true, y_pred)\n",
    "    recall = recall_measure(y_true, y_pred)\n",
    "    return 2 * ((precision * recall)/(precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "satellite-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 250)               132000    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 504       \n",
      "=================================================================\n",
      "Total params: 163,879\n",
      "Trainable params: 163,879\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features, n_outputs = training_X.shape[1], training_X.shape[2], training_y_encoded.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units = 125), input_shape = (n_timesteps, n_features)))\n",
    "model.add(Dropout(rate = 0.5))\n",
    "model.add(Dense(units = 125, activation = 'relu'))\n",
    "model.add(Dense(n_outputs, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['acc', f1, precision_measure, recall_measure])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bored-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1126710 samples, validate on 122696 samples\n",
      "Epoch 1/10\n",
      "1126710/1126710 [==============================] - 1340s 1ms/step - loss: 0.2001 - acc: 0.8831 - f1: 0.8800 - precision_measure: 0.8885 - recall_measure: 0.8727 - val_loss: 1.3778 - val_acc: 0.5955 - val_f1: 0.5921 - val_precision_measure: 0.5978 - val_recall_measure: 0.5870\n",
      "Epoch 2/10\n",
      "1126710/1126710 [==============================] - 1324s 1ms/step - loss: 0.0983 - acc: 0.9376 - f1: 0.9374 - precision_measure: 0.9398 - recall_measure: 0.9351 - val_loss: 1.6107 - val_acc: 0.6043 - val_f1: 0.6018 - val_precision_measure: 0.6049 - val_recall_measure: 0.5990\n",
      "Epoch 3/10\n",
      "1126710/1126710 [==============================] - 1284s 1ms/step - loss: 0.0691 - acc: 0.9541 - f1: 0.9540 - precision_measure: 0.9551 - recall_measure: 0.9530 - val_loss: 2.0734 - val_acc: 0.5978 - val_f1: 0.5960 - val_precision_measure: 0.5985 - val_recall_measure: 0.5936\n",
      "Epoch 4/10\n",
      "1126710/1126710 [==============================] - 1229s 1ms/step - loss: 0.0530 - acc: 0.9646 - f1: 0.9645 - precision_measure: 0.9652 - recall_measure: 0.9639 - val_loss: 2.3893 - val_acc: 0.6125 - val_f1: 0.6112 - val_precision_measure: 0.6137 - val_recall_measure: 0.6089\n",
      "Epoch 5/10\n",
      "1126710/1126710 [==============================] - 1187s 1ms/step - loss: 0.0437 - acc: 0.9706 - f1: 0.9706 - precision_measure: 0.9711 - recall_measure: 0.9702 - val_loss: 2.6459 - val_acc: 0.6136 - val_f1: 0.6127 - val_precision_measure: 0.6143 - val_recall_measure: 0.6111\n",
      "Epoch 6/10\n",
      "1126710/1126710 [==============================] - 1090s 968us/step - loss: 0.0372 - acc: 0.9753 - f1: 0.9753 - precision_measure: 0.9756 - recall_measure: 0.9750 - val_loss: 2.8200 - val_acc: 0.6013 - val_f1: 0.6001 - val_precision_measure: 0.6017 - val_recall_measure: 0.5985\n",
      "Epoch 7/10\n",
      "1126710/1126710 [==============================] - 1135s 1ms/step - loss: 0.0335 - acc: 0.9781 - f1: 0.9780 - precision_measure: 0.9783 - recall_measure: 0.9778 - val_loss: 3.1057 - val_acc: 0.6072 - val_f1: 0.6063 - val_precision_measure: 0.6080 - val_recall_measure: 0.6047\n",
      "Epoch 8/10\n",
      "1126710/1126710 [==============================] - 1137s 1ms/step - loss: 0.0290 - acc: 0.9811 - f1: 0.9811 - precision_measure: 0.9813 - recall_measure: 0.9808 - val_loss: 3.0116 - val_acc: 0.6168 - val_f1: 0.6159 - val_precision_measure: 0.6176 - val_recall_measure: 0.6143\n",
      "Epoch 9/10\n",
      "1126710/1126710 [==============================] - 1132s 1ms/step - loss: 0.0271 - acc: 0.9824 - f1: 0.9824 - precision_measure: 0.9826 - recall_measure: 0.9822 - val_loss: 3.3812 - val_acc: 0.6199 - val_f1: 0.6189 - val_precision_measure: 0.6203 - val_recall_measure: 0.6174\n",
      "Epoch 10/10\n",
      "1126710/1126710 [==============================] - 1094s 971us/step - loss: 0.0246 - acc: 0.9840 - f1: 0.9840 - precision_measure: 0.9842 - recall_measure: 0.9838 - val_loss: 3.4266 - val_acc: 0.6226 - val_f1: 0.6220 - val_precision_measure: 0.6231 - val_recall_measure: 0.6209\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_X, training_y_encoded, epochs = 10, batch_size = 64,\n",
    "                   validation_data = (val_X, val_y_encoded), class_weight = label_weights,\n",
    "                   verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "together-dance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91260/91260 [==============================] - 43s 470us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3247964921183148,\n",
       " 0.9340236686390533,\n",
       " 0.9339926425365145,\n",
       " 0.9342043334288789,\n",
       " 0.9337935568704799]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "failing-commitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122696/122696 [==============================] - 68s 556us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.426590192157689,\n",
       " 0.6226119840907609,\n",
       " 0.6218399181694828,\n",
       " 0.6228983695602109,\n",
       " 0.6208596857273261]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_X, val_y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "popular-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions using test data\n",
    "test_files = ['TestData/subject_009_01__x.csv', 'TestData/subject_010_01__x.csv', \n",
    "              'TestData/subject_011_01__x.csv', 'TestData/subject_012_01__x.csv']\n",
    "\n",
    "y_files = ['TestData/subject_009_01__y_time.csv', 'TestData/subject_010_01__y_time.csv',\n",
    "           'TestData/subject_011_01__y_time.csv', 'TestData/subject_012_01__y_time.csv']\n",
    "\n",
    "prediction_files = ['subject_009_01__y_prediction.csv', 'subject_010_01__y_prediction.csv',\n",
    "                    'subject_011_01__y_prediction.csv', 'subject_012_01__y_prediction.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "invisible-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37988/37988 [==============================] - 27s 719us/step\n",
      "9497\n",
      "49076/49076 [==============================] - 34s 688us/step\n",
      "12269\n",
      "51756/51756 [==============================] - 22s 431us/step\n",
      "12939\n",
      "45316/45316 [==============================] - 39s 863us/step\n",
      "11329\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(X, time_steps, step):\n",
    "    X_values = []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        value = X.iloc[i:(i + time_steps)].values\n",
    "        X_values.append(value)        \n",
    "    return np.array(X_values)\n",
    "\n",
    "def get_majority(y):\n",
    "    y_out = []\n",
    "    for i in range(0, y.shape[0], 4):\n",
    "        a = list(y[i:i+4])\n",
    "        y_out.append(max(a, key = a.count))\n",
    "    return np.array(y_out)\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    input_data = pd.read_csv(test_files[i])\n",
    "    df = scale_data(input_data, list(input_data.columns.values))\n",
    "    y_frame = pd.read_csv(y_files[i])\n",
    "    addl = y_frame.shape[0] * 4 - df.shape[0] + 30\n",
    "    addl_df = pd.DataFrame(df.iloc[-addl:])\n",
    "    df = df.append(addl_df)\n",
    "    X_test = create_dataset(df, 30, 1)\n",
    "    y_test = model.predict(X_test, batch_size = 64, verbose = 1)\n",
    "    y_test_bool = np.argmax(y_test, axis = 1)\n",
    "    y_actual = get_majority(y_test_bool)\n",
    "    print(y_actual.size)\n",
    "    y_series = pd.Series(y_actual)\n",
    "    y_series.to_csv(\"C2_predictions/\" + prediction_files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-country",
   "metadata": {},
   "source": [
    "finish testing subject_009_01__y_prediction.csv\n",
    "finish testing subject_010_01__y_prediction.csv\n",
    "finish testing subject_011_01__y_prediction.csv\n",
    "finish testing subject_012_01__y_prediction.csv\n",
    "# Final F1-score on hidden test data is 0.8602899935107109"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
