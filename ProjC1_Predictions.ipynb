{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "adjusted-simon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from keras) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from keras) (1.20.1)\n",
      "Collecting h5py\n",
      "  Using cached h5py-3.2.1-cp39-cp39-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyyaml, h5py, keras\n",
      "Successfully installed h5py-3.2.1 keras-2.4.3 pyyaml-5.4.1\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import imbalanced-learn\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-canal",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "velvet-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Based on the training data given, we are able to extract 7 attributes:\n",
    "    1. x accelerometer measurement\n",
    "    2. y accelerometer measurement\n",
    "    3. z accelerometer measurement\n",
    "    4. x gyroscope measurement\n",
    "    5. y gyroscope measurement\n",
    "    6. z gyroscope measurement\n",
    "    7. time stamp for accelerometer and gyroscope measures\n",
    "    \n",
    "    We start by creating a dataframe using the csv files provided for readability.\n",
    "    \n",
    "    @param x_file: contains the xyz accelerometers and xyz gyroscope measures from the lower limb\n",
    "    @param x_time_file: contain the time stamps for the accelerometer and gyroscope measures\n",
    "    @return dataframe of 7 attributes mentioned\n",
    "\"\"\"\n",
    "def create_dataframe_X(x_file, x_time_file):\n",
    "    df1 = pd.read_csv(x_file, sep = ',', names = ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df2 = pd.read_csv(x_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We have both the labels and the time stamps for the labels. We create a dataframe from these for\n",
    "    readability.\n",
    "    \n",
    "    @param y_file: contain the labels: \n",
    "        (0) indicates standing or walking in solid ground, \n",
    "        (1) indicates going down the stairs, \n",
    "        (2) indicates going up the stairs, and \n",
    "        (3) indicates walking on grass\n",
    "    @param y_time_file: contain the time stamps for the labels\n",
    "    @return dataframe of labels and time stamps\n",
    "\"\"\" \n",
    "def create_dataframe_Y(y_file, y_time_file):\n",
    "    df1 = pd.read_csv(y_file, names = ['Label'])\n",
    "    df2 = pd.read_csv(y_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We take the outputs of create_dataframe_X and create_dataframe_Y. In order to combine both of these\n",
    "    dataframes, we need look at the time intervals present for when the labels were assigned. The goal is\n",
    "    to return a dataframe that now has an eighth column in addition to the seven columns from the dataframe\n",
    "    from create_dataframe_X. Additionally, we know that x_frame contains more values than y_frame. We want to\n",
    "    map these labels accordingly. In the end, we drop data points that have missing values.\n",
    "    \n",
    "    @param x_frame: dataframe from create_dataframe_X\n",
    "    @param y_frame: dataframe from create_dataframe_Y\n",
    "    @return dataframe with 8 columns (7 attributes and label)\n",
    "\"\"\"\n",
    "def combine_frames(x_frame, y_frame):\n",
    "    # Change each dataframe column to a list for iterations\n",
    "    labels = y_frame['Label'].tolist()\n",
    "    time_stamp_y = y_frame['Time stamp'].tolist()\n",
    "    time_stamp_x = x_frame['Time stamp'].tolist()\n",
    "    \n",
    "    labels_for_x = [] # Create empty list to gather corresponding labels for x_frame\n",
    "    count = 0\n",
    "    for i in range(0, len(time_stamp_y)):\n",
    "        while (time_stamp_x[count] <= time_stamp_y[i]) and (count <= len(time_stamp_x)):\n",
    "            labels_for_x.append(labels[i])\n",
    "            count += 1\n",
    "        continue\n",
    "    \n",
    "    # Concatenate the dataframes\n",
    "    label_df = pd.DataFrame(labels_for_x, columns = ['Label']) # Convert list back to data frame\n",
    "    combined_frame = pd.concat([x_frame, label_df], axis = 1)\n",
    "    \n",
    "    # Drop missing values at the end\n",
    "    combined_frame = combined_frame.dropna()\n",
    "    \n",
    "    # Drop 'Time stamp' column\n",
    "    combined_frame = combined_frame.drop(columns = ['Time stamp'])\n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "intellectual-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X_acc     Y_acc     Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0      1.726654  9.619981  1.723327 -0.001997  0.067502  0.126057    0.0\n",
      "1      2.225759  9.493385  1.782374  0.008557  0.029333  0.073573    0.0\n",
      "2      2.010621  9.481603  1.770000 -0.004651  0.001009  0.062978    0.0\n",
      "3      1.614272  9.516440  1.798932  0.009519  0.024405  0.032554    0.0\n",
      "4      1.862582  9.353709  1.722649  0.007902  0.022794  0.020837    0.0\n",
      "...         ...       ...       ...       ...       ...       ...    ...\n",
      "70164  3.704972  8.586173  3.088743 -0.010505  0.009598 -0.004949    0.0\n",
      "70165  3.690854  8.759488  3.099146 -0.002501  0.001989  0.001526    0.0\n",
      "70166  3.939186  8.407883  3.049837  0.015672  0.011588  0.014313    0.0\n",
      "70167  3.762566  8.168921  3.062974  0.015675  0.007165  0.019624    0.0\n",
      "70168  3.729076  8.256303  3.034621 -0.005977  0.006976  0.006051    0.0\n",
      "\n",
      "[70169 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "x = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "y = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "combined = combine_frames(x, y)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "stock-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generating data frames from training data.\n",
    "\"\"\"\n",
    "# Subject_001_01\n",
    "df_x_1_1 = create_dataframe_X('TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_01__x_time.csv')\n",
    "df_y_1_1 = create_dataframe_Y('TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_01__y_time.csv')\n",
    "frame_1_1 = combine_frames(df_x_1_1, df_y_1_1)\n",
    "\n",
    "# Subject_001_02\n",
    "df_x_1_2 = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "df_y_1_2 = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "frame_1_2 = combine_frames(df_x_1_2, df_y_1_2)\n",
    "\n",
    "# Subject_001_03\n",
    "df_x_1_3 = create_dataframe_X('TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_03__x_time.csv')\n",
    "df_y_1_3 = create_dataframe_Y('TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_03__y_time.csv')\n",
    "frame_1_3 = combine_frames(df_x_1_3, df_y_1_3)\n",
    "\n",
    "# Subject_001_04\n",
    "df_x_1_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_1_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_1_4 = combine_frames(df_x_1_4, df_y_1_4)\n",
    "\n",
    "# Subject_001_05\n",
    "df_x_1_5 = create_dataframe_X('TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_05__x_time.csv')\n",
    "df_y_1_5 = create_dataframe_Y('TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_05__y_time.csv')\n",
    "frame_1_5 = combine_frames(df_x_1_5, df_y_1_5)\n",
    "\n",
    "# Subject_001_06\n",
    "df_x_1_6 = create_dataframe_X('TrainingData/subject_001_06__x.csv', 'TrainingData/subject_001_06__x_time.csv')\n",
    "df_y_1_6 = create_dataframe_Y('TrainingData/subject_001_06__y.csv', 'TrainingData/subject_001_06__y_time.csv')\n",
    "frame_1_6 = combine_frames(df_x_1_6, df_y_1_6)\n",
    "\n",
    "# Subject_001_07\n",
    "df_x_1_7 = create_dataframe_X('TrainingData/subject_001_07__x.csv', 'TrainingData/subject_001_07__x_time.csv')\n",
    "df_y_1_7 = create_dataframe_Y('TrainingData/subject_001_07__y.csv', 'TrainingData/subject_001_07__y_time.csv')\n",
    "frame_1_7 = combine_frames(df_x_1_7, df_y_1_7)\n",
    "\n",
    "# Subject_001_08\n",
    "df_x_1_8 = create_dataframe_X('TrainingData/subject_001_08__x.csv', 'TrainingData/subject_001_08__x_time.csv')\n",
    "df_y_1_8 = create_dataframe_Y('TrainingData/subject_001_08__y.csv', 'TrainingData/subject_001_08__y_time.csv')\n",
    "frame_1_8 = combine_frames(df_x_1_8, df_y_1_8)\n",
    "\n",
    "# Subject_002_01\n",
    "df_x_2_1 = create_dataframe_X('TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_01__x_time.csv')\n",
    "df_y_2_1 = create_dataframe_Y('TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_01__y_time.csv')\n",
    "frame_2_1 = combine_frames(df_x_2_1, df_y_2_1)\n",
    "\n",
    "# Subject_002_02\n",
    "df_x_2_2 = create_dataframe_X('TrainingData/subject_002_02__x.csv', 'TrainingData/subject_002_02__x_time.csv')\n",
    "df_y_2_2 = create_dataframe_Y('TrainingData/subject_002_02__y.csv', 'TrainingData/subject_002_02__y_time.csv')\n",
    "frame_2_2 = combine_frames(df_x_2_2, df_y_2_2)\n",
    "\n",
    "# Subject_002_03\n",
    "df_x_2_3 = create_dataframe_X('TrainingData/subject_002_03__x.csv', 'TrainingData/subject_002_03__x_time.csv')\n",
    "df_y_2_3 = create_dataframe_Y('TrainingData/subject_002_03__y.csv', 'TrainingData/subject_002_03__y_time.csv')\n",
    "frame_2_3 = combine_frames(df_x_2_3, df_y_2_3)\n",
    "\n",
    "# Subject_002_04\n",
    "df_x_2_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_2_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_2_4 = combine_frames(df_x_2_4, df_y_2_4)\n",
    "\n",
    "# Subject_002_05\n",
    "df_x_2_5 = create_dataframe_X('TrainingData/subject_002_05__x.csv', 'TrainingData/subject_002_05__x_time.csv')\n",
    "df_y_2_5 = create_dataframe_Y('TrainingData/subject_002_05__y.csv', 'TrainingData/subject_002_05__y_time.csv')\n",
    "frame_2_5 = combine_frames(df_x_2_5, df_y_2_5)\n",
    "\n",
    "# Subject_003_01\n",
    "df_x_3_1 = create_dataframe_X('TrainingData/subject_003_01__x.csv', 'TrainingData/subject_003_01__x_time.csv')\n",
    "df_y_3_1 = create_dataframe_Y('TrainingData/subject_003_01__y.csv', 'TrainingData/subject_003_01__y_time.csv')\n",
    "frame_3_1 = combine_frames(df_x_3_1, df_y_3_1)\n",
    "\n",
    "# Subject_003_02\n",
    "df_x_3_2 = create_dataframe_X('TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_02__x_time.csv')\n",
    "df_y_3_2 = create_dataframe_Y('TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_02__y_time.csv')\n",
    "frame_3_2 = combine_frames(df_x_3_2, df_y_3_2)\n",
    "\n",
    "# Subject_003_03\n",
    "df_x_3_3 = create_dataframe_X('TrainingData/subject_003_03__x.csv', 'TrainingData/subject_003_03__x_time.csv')\n",
    "df_y_3_3 = create_dataframe_Y('TrainingData/subject_003_03__y.csv', 'TrainingData/subject_003_03__y_time.csv')\n",
    "frame_3_3 = combine_frames(df_x_3_3, df_y_3_3)\n",
    "\n",
    "# Subject_004_01\n",
    "df_x_4_1 = create_dataframe_X('TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_01__x_time.csv')\n",
    "df_y_4_1 = create_dataframe_Y('TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_01__y_time.csv')\n",
    "frame_4_1 = combine_frames(df_x_4_1, df_y_4_1)\n",
    "\n",
    "# Subject_004_02\n",
    "df_x_4_2 = create_dataframe_X('TrainingData/subject_004_02__x.csv', 'TrainingData/subject_004_02__x_time.csv')\n",
    "df_y_4_2 = create_dataframe_Y('TrainingData/subject_004_02__y.csv', 'TrainingData/subject_004_02__y_time.csv')\n",
    "frame_4_2 = combine_frames(df_x_4_2, df_y_4_2)\n",
    "\n",
    "# Subject_005_01\n",
    "df_x_5_1 = create_dataframe_X('TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_01__x_time.csv')\n",
    "df_y_5_1 = create_dataframe_Y('TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_01__y_time.csv')\n",
    "frame_5_1 = combine_frames(df_x_5_1, df_y_5_1)\n",
    "\n",
    "# Subject_005_02\n",
    "df_x_5_2 = create_dataframe_X('TrainingData/subject_005_02__x.csv', 'TrainingData/subject_005_02__x_time.csv')\n",
    "df_y_5_2 = create_dataframe_Y('TrainingData/subject_005_02__y.csv', 'TrainingData/subject_005_02__y_time.csv')\n",
    "frame_5_2 = combine_frames(df_x_5_2, df_y_5_2)\n",
    "\n",
    "# Subject_005_03\n",
    "df_x_5_3 = create_dataframe_X('TrainingData/subject_005_03__x.csv', 'TrainingData/subject_005_03__x_time.csv')\n",
    "df_y_5_3 = create_dataframe_Y('TrainingData/subject_005_03__y.csv', 'TrainingData/subject_005_03__y_time.csv')\n",
    "frame_5_3 = combine_frames(df_x_5_3, df_y_5_3)\n",
    "\n",
    "# Subject_006_01\n",
    "df_x_6_1 = create_dataframe_X('TrainingData/subject_006_01__x.csv', 'TrainingData/subject_006_01__x_time.csv')\n",
    "df_y_6_1 = create_dataframe_Y('TrainingData/subject_006_01__y.csv', 'TrainingData/subject_006_01__y_time.csv')\n",
    "frame_6_1 = combine_frames(df_x_6_1, df_y_6_1)\n",
    "\n",
    "# Subject_006_02\n",
    "df_x_6_2 = create_dataframe_X('TrainingData/subject_006_02__x.csv', 'TrainingData/subject_006_02__x_time.csv')\n",
    "df_y_6_2 = create_dataframe_Y('TrainingData/subject_006_02__y.csv', 'TrainingData/subject_006_02__y_time.csv')\n",
    "frame_6_2 = combine_frames(df_x_6_2, df_y_6_2)\n",
    "\n",
    "# Subject_006_03\n",
    "df_x_6_3 = create_dataframe_X('TrainingData/subject_006_03__x.csv', 'TrainingData/subject_006_03__x_time.csv')\n",
    "df_y_6_3 = create_dataframe_Y('TrainingData/subject_006_03__y.csv', 'TrainingData/subject_006_03__y_time.csv')\n",
    "frame_6_3 = combine_frames(df_x_6_3, df_y_6_3)\n",
    "\n",
    "# Subject_007_01\n",
    "df_x_7_1 = create_dataframe_X('TrainingData/subject_007_01__x.csv', 'TrainingData/subject_007_01__x_time.csv')\n",
    "df_y_7_1 = create_dataframe_Y('TrainingData/subject_007_01__y.csv', 'TrainingData/subject_007_01__y_time.csv')\n",
    "frame_7_1 = combine_frames(df_x_7_1, df_y_7_1)\n",
    "\n",
    "# Subject_007_02\n",
    "df_x_7_2 = create_dataframe_X('TrainingData/subject_007_02__x.csv', 'TrainingData/subject_007_02__x_time.csv')\n",
    "df_y_7_2 = create_dataframe_Y('TrainingData/subject_007_02__y.csv', 'TrainingData/subject_007_02__y_time.csv')\n",
    "frame_7_2 = combine_frames(df_x_7_2, df_y_7_2)\n",
    "\n",
    "# Subject_007_03\n",
    "df_x_7_3 = create_dataframe_X('TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_03__x_time.csv')\n",
    "df_y_7_3 = create_dataframe_Y('TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_03__y_time.csv')\n",
    "frame_7_3 = combine_frames(df_x_7_3, df_y_7_3)\n",
    "\n",
    "# Subject_007_04\n",
    "df_x_7_4 = create_dataframe_X('TrainingData/subject_007_04__x.csv', 'TrainingData/subject_007_04__x_time.csv')\n",
    "df_y_7_4 = create_dataframe_Y('TrainingData/subject_007_04__y.csv', 'TrainingData/subject_007_04__y_time.csv')\n",
    "frame_7_4 = combine_frames(df_x_7_4, df_y_7_4)\n",
    "\n",
    "# Subject_008_01\n",
    "df_x_8_1 = create_dataframe_X('TrainingData/subject_008_01__x.csv', 'TrainingData/subject_008_01__x_time.csv')\n",
    "df_y_8_1 = create_dataframe_Y('TrainingData/subject_008_01__y.csv', 'TrainingData/subject_008_01__y_time.csv')\n",
    "frame_8_1 = combine_frames(df_x_8_1, df_y_8_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "authentic-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Combining all data frames.\n",
    "\"\"\"\n",
    "frame_list = [frame_1_1, frame_1_2, frame_1_3, frame_1_4, frame_1_5, frame_1_6, frame_1_7, frame_1_8,\n",
    "             frame_2_1, frame_2_2, frame_2_3, frame_2_4, frame_2_5,\n",
    "             frame_3_1, frame_3_2, frame_3_3,\n",
    "             frame_4_1, frame_4_2,\n",
    "             frame_5_1, frame_5_2, frame_5_3,\n",
    "             frame_6_1, frame_6_2, frame_6_3,\n",
    "             frame_7_1, frame_7_2, frame_7_3, frame_7_4,\n",
    "             frame_8_1]\n",
    "data = pd.concat(frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "exotic-homeless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X_acc     Y_acc     Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0      4.435275  8.196063  2.974488  0.014215 -0.039157 -0.016744    0.0\n",
      "1      4.186920  8.344455  2.908057  0.005771 -0.004480 -0.003345    0.0\n",
      "2      4.544637  8.408659  2.890000  0.007967  0.022412  0.001159    0.0\n",
      "3      4.849308  8.411614  2.900692  0.027778 -0.010670 -0.014223    0.0\n",
      "4      4.509190  8.118649  2.847298  0.021577 -0.045498 -0.021111    0.0\n",
      "...         ...       ...       ...       ...       ...       ...    ...\n",
      "48132  2.098301  8.893398 -3.510000  0.001195  0.000335  0.001027    0.0\n",
      "48133  2.072244  8.908878 -3.500000  0.001351  0.001191  0.001031    0.0\n",
      "48134  2.085123  8.915123 -3.520000  0.001918 -0.001147  0.000000    0.0\n",
      "48135  2.083774  8.910000 -3.538981 -0.002015 -0.004099  0.001042    0.0\n",
      "48136  2.111447  8.908553 -3.535724  0.000183 -0.001673  0.001856    0.0\n",
      "\n",
      "[1345061 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "romantic-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Create X and y\n",
    "X = data[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr']]\n",
    "y = data['Label']\n",
    "#oversample = SMOTE()\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "competitive-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr\n",
      "0        0.321669  10.295200   4.258973  2.494807  0.073337 -0.966376\n",
      "1       -4.914809  10.531940   3.010600 -2.201648  0.202660  0.041513\n",
      "2        2.668823   8.943054   1.229262 -1.607988  0.111795 -0.026149\n",
      "3      -10.848110   4.496461   5.922262 -2.238039  0.211609  0.832803\n",
      "4        3.914291   7.825164   2.354546 -0.831248 -0.551987 -0.259325\n",
      "...           ...        ...        ...       ...       ...       ...\n",
      "233803  -8.526870   9.205021   7.646483 -1.475977 -1.290600 -0.301872\n",
      "233804  -1.152478   8.927959   9.071304 -2.094907  1.579927  0.227422\n",
      "233805  -2.547585   6.239240  12.170110  1.000152  0.790138 -0.569987\n",
      "233806   1.053528  13.871860   0.063724 -3.259116 -2.046253  0.696361\n",
      "233807  -5.458344   2.140917  10.254290  6.457951 -0.729170  1.101775\n",
      "\n",
      "[233808 rows x 6 columns]\n",
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "233803    3.0\n",
      "233804    3.0\n",
      "233805    3.0\n",
      "233806    3.0\n",
      "233807    3.0\n",
      "Name: Label, Length: 233808, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "print(X_resampled)\n",
    "print(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "guilty-stuart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0        0.321669  10.295200   4.258973  2.494807  0.073337 -0.966376    0.0\n",
      "1       -4.914809  10.531940   3.010600 -2.201648  0.202660  0.041513    0.0\n",
      "2        2.668823   8.943054   1.229262 -1.607988  0.111795 -0.026149    0.0\n",
      "3      -10.848110   4.496461   5.922262 -2.238039  0.211609  0.832803    0.0\n",
      "4        3.914291   7.825164   2.354546 -0.831248 -0.551987 -0.259325    0.0\n",
      "...           ...        ...        ...       ...       ...       ...    ...\n",
      "233803  -8.526870   9.205021   7.646483 -1.475977 -1.290600 -0.301872    3.0\n",
      "233804  -1.152478   8.927959   9.071304 -2.094907  1.579927  0.227422    3.0\n",
      "233805  -2.547585   6.239240  12.170110  1.000152  0.790138 -0.569987    3.0\n",
      "233806   1.053528  13.871860   0.063724 -3.259116 -2.046253  0.696361    3.0\n",
      "233807  -5.458344   2.140917  10.254290  6.457951 -0.729170  1.101775    3.0\n",
      "\n",
      "[233808 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data_resampled = pd.concat([X_resampled, y_resampled], axis = 1)\n",
    "print(data_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dependence",
   "metadata": {},
   "source": [
    "# Training, Validation, Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "color-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "99374   0.443280   2.371991  11.824440  5.485730 -2.511950 -0.597672    1.0\n",
      "54135  -2.832240   9.580369   4.016701 -0.882984 -1.398961 -0.539089    0.0\n",
      "147319  9.243908  -6.753744   5.495813  3.885012 -0.866962 -0.969324    2.0\n",
      "228455  6.076344   5.857147   7.815936 -1.373564 -0.952581 -0.321045    3.0\n",
      "213840  3.544153  -0.824973   7.932295  5.702078  2.552324  2.608943    3.0\n",
      "...          ...        ...        ...       ...       ...       ...    ...\n",
      "188023  7.543629   8.334155  12.425790 -4.262642  2.359509  1.674000    3.0\n",
      "126153  6.413435   4.418072  10.652710 -2.495509 -0.764505 -1.058842    2.0\n",
      "189727  4.970114  18.413130   1.925700 -4.149369 -1.792227 -0.524522    3.0\n",
      "3361    2.103770   0.991507  13.809880 -0.598645  0.030965 -0.142732    0.0\n",
      "82986   1.716331  17.573390   7.859266 -1.511394  0.912361 -0.011570    1.0\n",
      "\n",
      "[175356 rows x 7 columns]\n",
      "Validation:\n",
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "109065  1.340281  19.808290  18.130480 -4.541178 -1.831368  0.112899    1.0\n",
      "44364  -8.539748   4.582397  14.544850  6.889263  0.400195 -1.299423    0.0\n",
      "217260  3.371894   3.916124  13.343100  7.390979 -0.746827  0.277075    3.0\n",
      "63374  -2.138583   7.195535   2.580936 -1.638861 -0.209257  0.014403    1.0\n",
      "181513 -6.966177   8.878353   5.598353 -0.772569 -1.955664 -0.453347    3.0\n",
      "...          ...        ...        ...       ...       ...       ...    ...\n",
      "118975 -1.140397   7.653902   5.126128  0.615365 -0.779301  0.075206    2.0\n",
      "232323 -1.253060   7.831704   1.908522 -1.542577 -0.173245  0.169999    3.0\n",
      "112411  3.333670   3.381671   6.915709  1.356015  1.003957  0.422088    1.0\n",
      "84116  -2.759807   2.870911   5.639007  4.469371  3.492041  1.472650    1.0\n",
      "148788  1.847418   0.546901   5.934170  2.227376  0.792258  0.209967    2.0\n",
      "\n",
      "[29226 rows x 7 columns]\n",
      "Test:\n",
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "21      2.283801  14.918490  10.697530 -1.511189  3.630429  0.596639    0.0\n",
      "37      0.545069   8.791847   2.719765 -0.688419 -0.134145 -0.128697    0.0\n",
      "43      1.927812   9.448925   3.091286  0.125508 -0.272825 -0.027163    0.0\n",
      "46     -5.403930   7.875444   3.436919 -0.671170 -2.523009 -0.565114    0.0\n",
      "55      6.595331  10.085760   1.164397 -1.492771  0.562250  0.327917    0.0\n",
      "...          ...        ...        ...       ...       ...       ...    ...\n",
      "233752  7.481142   1.636054  10.729130 -3.136037  1.572726  0.349721    3.0\n",
      "233756 -4.145525   9.276828   7.439458 -3.423434  1.573703  1.258922    3.0\n",
      "233761  2.138197  10.914870  -0.846361 -1.624774  0.072213 -0.306671    3.0\n",
      "233783  3.553030  11.240610   2.777171 -2.718525 -0.423632 -0.296170    3.0\n",
      "233792  3.911373   1.110181  10.930240  6.532366  2.768746  0.224613    3.0\n",
      "\n",
      "[29226 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Creating training, validation, and test sets from a data frame.\n",
    "    \n",
    "    @param frame: dataframe passed in\n",
    "    @return training, validation, and test sets created from frame passed in\n",
    "\"\"\"\n",
    "def training_validation_test_split(frame):\n",
    "    frame_copy = frame.copy()\n",
    "    training_set = frame_copy.sample(frac = 0.75, random_state = 0)\n",
    "    val_temp = frame_copy.drop(training_set.index)\n",
    "    validation_set = val_temp.sample(frac = 0.5, random_state = 0)\n",
    "    test_set = val_temp.drop(validation_set.index)\n",
    "    return training_set, validation_set, test_set\n",
    "\n",
    "training, val, test = training_validation_test_split(data_resampled)\n",
    "\n",
    "print(\"Training:\\n\", training)\n",
    "print(\"Validation:\\n\", val)\n",
    "print(\"Test:\\n\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "closed-webster",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-70d283a5a998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "motivated-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<1.9.0,>=1.8.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (3.15.6)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (1.36.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (0.9999999)\n",
      "Requirement already satisfied: bleach==1.5.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#import tensorflow as tf\n",
    "\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-glass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
