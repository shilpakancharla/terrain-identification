{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "adjusted-simon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from imbalanced-learn) (1.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.8.0\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import imbalanced-learn\n",
    "#!{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-enemy",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "velvet-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Based on the training data given, we are able to extract 7 attributes:\n",
    "    1. x accelerometer measurement\n",
    "    2. y accelerometer measurement\n",
    "    3. z accelerometer measurement\n",
    "    4. x gyroscope measurement\n",
    "    5. y gyroscope measurement\n",
    "    6. z gyroscope measurement\n",
    "    7. time stamp for accelerometer and gyroscope measures\n",
    "    \n",
    "    We start by creating a dataframe using the csv files provided for readability.\n",
    "    \n",
    "    @param x_file: contains the xyz accelerometers and xyz gyroscope measures from the lower limb\n",
    "    @param x_time_file: contain the time stamps for the accelerometer and gyroscope measures\n",
    "    @return dataframe of 7 attributes mentioned\n",
    "\"\"\"\n",
    "def create_dataframe_X(x_file, x_time_file):\n",
    "    df1 = pd.read_csv(x_file, sep = ',', names = ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df2 = pd.read_csv(x_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We have both the labels and the time stamps for the labels. We create a dataframe from these for\n",
    "    readability.\n",
    "    \n",
    "    @param y_file: contain the labels: \n",
    "        (0) indicates standing or walking in solid ground, \n",
    "        (1) indicates going down the stairs, \n",
    "        (2) indicates going up the stairs, and \n",
    "        (3) indicates walking on grass\n",
    "    @param y_time_file: contain the time stamps for the labels\n",
    "    @return dataframe of labels and time stamps\n",
    "\"\"\" \n",
    "def create_dataframe_Y(y_file, y_time_file):\n",
    "    df1 = pd.read_csv(y_file, names = ['Label'])\n",
    "    df2 = pd.read_csv(y_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We take the outputs of create_dataframe_X and create_dataframe_Y. In order to combine both of these\n",
    "    dataframes, we need look at the time intervals present for when the labels were assigned. The goal is\n",
    "    to return a dataframe that now has an eighth column in addition to the seven columns from the dataframe\n",
    "    from create_dataframe_X. Additionally, we know that x_frame contains more values than y_frame. We want to\n",
    "    map these labels accordingly. In the end, we drop data points that have missing values.\n",
    "    \n",
    "    @param x_frame: dataframe from create_dataframe_X\n",
    "    @param y_frame: dataframe from create_dataframe_Y\n",
    "    @return dataframe with 8 columns (7 attributes and label)\n",
    "\"\"\"\n",
    "def combine_frames(x_frame, y_frame):\n",
    "    # Change each dataframe column to a list for iterations\n",
    "    labels = y_frame['Label'].tolist()\n",
    "    time_stamp_y = y_frame['Time stamp'].tolist()\n",
    "    time_stamp_x = x_frame['Time stamp'].tolist()\n",
    "    \n",
    "    labels_for_x = [] # Create empty list to gather corresponding labels for x_frame\n",
    "    count = 0\n",
    "    for i in range(0, len(time_stamp_y)):\n",
    "        while (time_stamp_x[count] <= time_stamp_y[i]) and (count <= len(time_stamp_x)):\n",
    "            labels_for_x.append(labels[i])\n",
    "            count += 1\n",
    "        continue\n",
    "    \n",
    "    # Concatenate the dataframes\n",
    "    label_df = pd.DataFrame(labels_for_x, columns = ['Label']) # Convert list back to data frame\n",
    "    combined_frame = pd.concat([x_frame, label_df], axis = 1)\n",
    "    \n",
    "    # Drop missing values at the end\n",
    "    combined_frame = combined_frame.dropna()\n",
    "    \n",
    "    # Drop 'Time stamp' column\n",
    "    combined_frame = combined_frame.drop(columns = ['Time stamp'])\n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "severe-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X_acc     Y_acc     Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0      1.726654  9.619981  1.723327 -0.001997  0.067502  0.126057    0.0\n",
      "1      2.225759  9.493385  1.782374  0.008557  0.029333  0.073573    0.0\n",
      "2      2.010621  9.481603  1.770000 -0.004651  0.001009  0.062978    0.0\n",
      "3      1.614272  9.516440  1.798932  0.009519  0.024405  0.032554    0.0\n",
      "4      1.862582  9.353709  1.722649  0.007902  0.022794  0.020837    0.0\n",
      "...         ...       ...       ...       ...       ...       ...    ...\n",
      "70164  3.704972  8.586173  3.088743 -0.010505  0.009598 -0.004949    0.0\n",
      "70165  3.690854  8.759488  3.099146 -0.002501  0.001989  0.001526    0.0\n",
      "70166  3.939186  8.407883  3.049837  0.015672  0.011588  0.014313    0.0\n",
      "70167  3.762566  8.168921  3.062974  0.015675  0.007165  0.019624    0.0\n",
      "70168  3.729076  8.256303  3.034621 -0.005977  0.006976  0.006051    0.0\n",
      "\n",
      "[70169 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "x = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "y = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "combined = combine_frames(x, y)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "stock-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generating data frames from training data.\n",
    "\"\"\"\n",
    "# Subject_001_01\n",
    "df_x_1_1 = create_dataframe_X('TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_01__x_time.csv')\n",
    "df_y_1_1 = create_dataframe_Y('TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_01__y_time.csv')\n",
    "frame_1_1 = combine_frames(df_x_1_1, df_y_1_1)\n",
    "\n",
    "# Subject_001_02\n",
    "df_x_1_2 = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "df_y_1_2 = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "frame_1_2 = combine_frames(df_x_1_2, df_y_1_2)\n",
    "\n",
    "# Subject_001_03\n",
    "df_x_1_3 = create_dataframe_X('TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_03__x_time.csv')\n",
    "df_y_1_3 = create_dataframe_Y('TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_03__y_time.csv')\n",
    "frame_1_3 = combine_frames(df_x_1_3, df_y_1_3)\n",
    "\n",
    "# Subject_001_04\n",
    "df_x_1_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_1_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_1_4 = combine_frames(df_x_1_4, df_y_1_4)\n",
    "\n",
    "# Subject_001_05\n",
    "df_x_1_5 = create_dataframe_X('TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_05__x_time.csv')\n",
    "df_y_1_5 = create_dataframe_Y('TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_05__y_time.csv')\n",
    "frame_1_5 = combine_frames(df_x_1_5, df_y_1_5)\n",
    "\n",
    "# Subject_001_06\n",
    "df_x_1_6 = create_dataframe_X('TrainingData/subject_001_06__x.csv', 'TrainingData/subject_001_06__x_time.csv')\n",
    "df_y_1_6 = create_dataframe_Y('TrainingData/subject_001_06__y.csv', 'TrainingData/subject_001_06__y_time.csv')\n",
    "frame_1_6 = combine_frames(df_x_1_6, df_y_1_6)\n",
    "\n",
    "# Subject_001_07\n",
    "df_x_1_7 = create_dataframe_X('TrainingData/subject_001_07__x.csv', 'TrainingData/subject_001_07__x_time.csv')\n",
    "df_y_1_7 = create_dataframe_Y('TrainingData/subject_001_07__y.csv', 'TrainingData/subject_001_07__y_time.csv')\n",
    "frame_1_7 = combine_frames(df_x_1_7, df_y_1_7)\n",
    "\n",
    "# Subject_001_08\n",
    "df_x_1_8 = create_dataframe_X('TrainingData/subject_001_08__x.csv', 'TrainingData/subject_001_08__x_time.csv')\n",
    "df_y_1_8 = create_dataframe_Y('TrainingData/subject_001_08__y.csv', 'TrainingData/subject_001_08__y_time.csv')\n",
    "frame_1_8 = combine_frames(df_x_1_8, df_y_1_8)\n",
    "\n",
    "# Subject_002_01\n",
    "df_x_2_1 = create_dataframe_X('TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_01__x_time.csv')\n",
    "df_y_2_1 = create_dataframe_Y('TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_01__y_time.csv')\n",
    "frame_2_1 = combine_frames(df_x_2_1, df_y_2_1)\n",
    "\n",
    "# Subject_002_02\n",
    "df_x_2_2 = create_dataframe_X('TrainingData/subject_002_02__x.csv', 'TrainingData/subject_002_02__x_time.csv')\n",
    "df_y_2_2 = create_dataframe_Y('TrainingData/subject_002_02__y.csv', 'TrainingData/subject_002_02__y_time.csv')\n",
    "frame_2_2 = combine_frames(df_x_2_2, df_y_2_2)\n",
    "\n",
    "# Subject_002_03\n",
    "df_x_2_3 = create_dataframe_X('TrainingData/subject_002_03__x.csv', 'TrainingData/subject_002_03__x_time.csv')\n",
    "df_y_2_3 = create_dataframe_Y('TrainingData/subject_002_03__y.csv', 'TrainingData/subject_002_03__y_time.csv')\n",
    "frame_2_3 = combine_frames(df_x_2_3, df_y_2_3)\n",
    "\n",
    "# Subject_002_04\n",
    "df_x_2_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_2_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_2_4 = combine_frames(df_x_2_4, df_y_2_4)\n",
    "\n",
    "# Subject_002_05\n",
    "df_x_2_5 = create_dataframe_X('TrainingData/subject_002_05__x.csv', 'TrainingData/subject_002_05__x_time.csv')\n",
    "df_y_2_5 = create_dataframe_Y('TrainingData/subject_002_05__y.csv', 'TrainingData/subject_002_05__y_time.csv')\n",
    "frame_2_5 = combine_frames(df_x_2_5, df_y_2_5)\n",
    "\n",
    "# Subject_003_01\n",
    "df_x_3_1 = create_dataframe_X('TrainingData/subject_003_01__x.csv', 'TrainingData/subject_003_01__x_time.csv')\n",
    "df_y_3_1 = create_dataframe_Y('TrainingData/subject_003_01__y.csv', 'TrainingData/subject_003_01__y_time.csv')\n",
    "frame_3_1 = combine_frames(df_x_3_1, df_y_3_1)\n",
    "\n",
    "# Subject_003_02\n",
    "df_x_3_2 = create_dataframe_X('TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_02__x_time.csv')\n",
    "df_y_3_2 = create_dataframe_Y('TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_02__y_time.csv')\n",
    "frame_3_2 = combine_frames(df_x_3_2, df_y_3_2)\n",
    "\n",
    "# Subject_003_03\n",
    "df_x_3_3 = create_dataframe_X('TrainingData/subject_003_03__x.csv', 'TrainingData/subject_003_03__x_time.csv')\n",
    "df_y_3_3 = create_dataframe_Y('TrainingData/subject_003_03__y.csv', 'TrainingData/subject_003_03__y_time.csv')\n",
    "frame_3_3 = combine_frames(df_x_3_3, df_y_3_3)\n",
    "\n",
    "# Subject_004_01\n",
    "df_x_4_1 = create_dataframe_X('TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_01__x_time.csv')\n",
    "df_y_4_1 = create_dataframe_Y('TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_01__y_time.csv')\n",
    "frame_4_1 = combine_frames(df_x_4_1, df_y_4_1)\n",
    "\n",
    "# Subject_004_02\n",
    "df_x_4_2 = create_dataframe_X('TrainingData/subject_004_02__x.csv', 'TrainingData/subject_004_02__x_time.csv')\n",
    "df_y_4_2 = create_dataframe_Y('TrainingData/subject_004_02__y.csv', 'TrainingData/subject_004_02__y_time.csv')\n",
    "frame_4_2 = combine_frames(df_x_4_2, df_y_4_2)\n",
    "\n",
    "# Subject_005_01\n",
    "df_x_5_1 = create_dataframe_X('TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_01__x_time.csv')\n",
    "df_y_5_1 = create_dataframe_Y('TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_01__y_time.csv')\n",
    "frame_5_1 = combine_frames(df_x_5_1, df_y_5_1)\n",
    "\n",
    "# Subject_005_02\n",
    "df_x_5_2 = create_dataframe_X('TrainingData/subject_005_02__x.csv', 'TrainingData/subject_005_02__x_time.csv')\n",
    "df_y_5_2 = create_dataframe_Y('TrainingData/subject_005_02__y.csv', 'TrainingData/subject_005_02__y_time.csv')\n",
    "frame_5_2 = combine_frames(df_x_5_2, df_y_5_2)\n",
    "\n",
    "# Subject_005_03\n",
    "df_x_5_3 = create_dataframe_X('TrainingData/subject_005_03__x.csv', 'TrainingData/subject_005_03__x_time.csv')\n",
    "df_y_5_3 = create_dataframe_Y('TrainingData/subject_005_03__y.csv', 'TrainingData/subject_005_03__y_time.csv')\n",
    "frame_5_3 = combine_frames(df_x_5_3, df_y_5_3)\n",
    "\n",
    "# Subject_006_01\n",
    "df_x_6_1 = create_dataframe_X('TrainingData/subject_006_01__x.csv', 'TrainingData/subject_006_01__x_time.csv')\n",
    "df_y_6_1 = create_dataframe_Y('TrainingData/subject_006_01__y.csv', 'TrainingData/subject_006_01__y_time.csv')\n",
    "frame_6_1 = combine_frames(df_x_6_1, df_y_6_1)\n",
    "\n",
    "# Subject_006_02\n",
    "df_x_6_2 = create_dataframe_X('TrainingData/subject_006_02__x.csv', 'TrainingData/subject_006_02__x_time.csv')\n",
    "df_y_6_2 = create_dataframe_Y('TrainingData/subject_006_02__y.csv', 'TrainingData/subject_006_02__y_time.csv')\n",
    "frame_6_2 = combine_frames(df_x_6_2, df_y_6_2)\n",
    "\n",
    "# Subject_006_03\n",
    "df_x_6_3 = create_dataframe_X('TrainingData/subject_006_03__x.csv', 'TrainingData/subject_006_03__x_time.csv')\n",
    "df_y_6_3 = create_dataframe_Y('TrainingData/subject_006_03__y.csv', 'TrainingData/subject_006_03__y_time.csv')\n",
    "frame_6_3 = combine_frames(df_x_6_3, df_y_6_3)\n",
    "\n",
    "# Subject_007_01\n",
    "df_x_7_1 = create_dataframe_X('TrainingData/subject_007_01__x.csv', 'TrainingData/subject_007_01__x_time.csv')\n",
    "df_y_7_1 = create_dataframe_Y('TrainingData/subject_007_01__y.csv', 'TrainingData/subject_007_01__y_time.csv')\n",
    "frame_7_1 = combine_frames(df_x_7_1, df_y_7_1)\n",
    "\n",
    "# Subject_007_02\n",
    "df_x_7_2 = create_dataframe_X('TrainingData/subject_007_02__x.csv', 'TrainingData/subject_007_02__x_time.csv')\n",
    "df_y_7_2 = create_dataframe_Y('TrainingData/subject_007_02__y.csv', 'TrainingData/subject_007_02__y_time.csv')\n",
    "frame_7_2 = combine_frames(df_x_7_2, df_y_7_2)\n",
    "\n",
    "# Subject_007_03\n",
    "df_x_7_3 = create_dataframe_X('TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_03__x_time.csv')\n",
    "df_y_7_3 = create_dataframe_Y('TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_03__y_time.csv')\n",
    "frame_7_3 = combine_frames(df_x_7_3, df_y_7_3)\n",
    "\n",
    "# Subject_007_04\n",
    "df_x_7_4 = create_dataframe_X('TrainingData/subject_007_04__x.csv', 'TrainingData/subject_007_04__x_time.csv')\n",
    "df_y_7_4 = create_dataframe_Y('TrainingData/subject_007_04__y.csv', 'TrainingData/subject_007_04__y_time.csv')\n",
    "frame_7_4 = combine_frames(df_x_7_4, df_y_7_4)\n",
    "\n",
    "# Subject_008_01\n",
    "df_x_8_1 = create_dataframe_X('TrainingData/subject_008_01__x.csv', 'TrainingData/subject_008_01__x_time.csv')\n",
    "df_y_8_1 = create_dataframe_Y('TrainingData/subject_008_01__y.csv', 'TrainingData/subject_008_01__y_time.csv')\n",
    "frame_8_1 = combine_frames(df_x_8_1, df_y_8_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "canadian-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Combining all data frames.\n",
    "\"\"\"\n",
    "frame_list = [frame_1_1, frame_1_2, frame_1_3, frame_1_4, frame_1_5, frame_1_6, frame_1_7, frame_1_8,\n",
    "             frame_2_1, frame_2_2, frame_2_3, frame_2_4, frame_2_5,\n",
    "             frame_3_1, frame_3_2, frame_3_3,\n",
    "             frame_4_1, frame_4_2,\n",
    "             frame_5_1, frame_5_2, frame_5_3,\n",
    "             frame_6_1, frame_6_2, frame_6_3,\n",
    "             frame_7_1, frame_7_2, frame_7_3, frame_7_4,\n",
    "             frame_8_1]\n",
    "data = pd.concat(frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "frozen-antibody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X_acc     Y_acc     Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0      4.435275  8.196063  2.974488  0.014215 -0.039157 -0.016744    0.0\n",
      "1      4.186920  8.344455  2.908057  0.005771 -0.004480 -0.003345    0.0\n",
      "2      4.544637  8.408659  2.890000  0.007967  0.022412  0.001159    0.0\n",
      "3      4.849308  8.411614  2.900692  0.027778 -0.010670 -0.014223    0.0\n",
      "4      4.509190  8.118649  2.847298  0.021577 -0.045498 -0.021111    0.0\n",
      "...         ...       ...       ...       ...       ...       ...    ...\n",
      "48132  2.098301  8.893398 -3.510000  0.001195  0.000335  0.001027    0.0\n",
      "48133  2.072244  8.908878 -3.500000  0.001351  0.001191  0.001031    0.0\n",
      "48134  2.085123  8.915123 -3.520000  0.001918 -0.001147  0.000000    0.0\n",
      "48135  2.083774  8.910000 -3.538981 -0.002015 -0.004099  0.001042    0.0\n",
      "48136  2.111447  8.908553 -3.535724  0.000183 -0.001673  0.001856    0.0\n",
      "\n",
      "[1345061 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "seasonal-truck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Create X and y\n",
    "X = data[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr']]\n",
    "y = data['Label']\n",
    "#oversample = SMOTE()\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "optimum-miller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr\n",
      "0        0.321669  10.295200   4.258973  2.494807  0.073337 -0.966376\n",
      "1       -4.914809  10.531940   3.010600 -2.201648  0.202660  0.041513\n",
      "2        2.668823   8.943054   1.229262 -1.607988  0.111795 -0.026149\n",
      "3      -10.848110   4.496461   5.922262 -2.238039  0.211609  0.832803\n",
      "4        3.914291   7.825164   2.354546 -0.831248 -0.551987 -0.259325\n",
      "...           ...        ...        ...       ...       ...       ...\n",
      "233803  -8.526870   9.205021   7.646483 -1.475977 -1.290600 -0.301872\n",
      "233804  -1.152478   8.927959   9.071304 -2.094907  1.579927  0.227422\n",
      "233805  -2.547585   6.239240  12.170110  1.000152  0.790138 -0.569987\n",
      "233806   1.053528  13.871860   0.063724 -3.259116 -2.046253  0.696361\n",
      "233807  -5.458344   2.140917  10.254290  6.457951 -0.729170  1.101775\n",
      "\n",
      "[233808 rows x 6 columns]\n",
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "233803    3.0\n",
      "233804    3.0\n",
      "233805    3.0\n",
      "233806    3.0\n",
      "233807    3.0\n",
      "Name: Label, Length: 233808, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "print(X_resampled)\n",
    "print(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "polish-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0        0.321669  10.295200   4.258973  2.494807  0.073337 -0.966376    0.0\n",
      "1       -4.914809  10.531940   3.010600 -2.201648  0.202660  0.041513    0.0\n",
      "2        2.668823   8.943054   1.229262 -1.607988  0.111795 -0.026149    0.0\n",
      "3      -10.848110   4.496461   5.922262 -2.238039  0.211609  0.832803    0.0\n",
      "4        3.914291   7.825164   2.354546 -0.831248 -0.551987 -0.259325    0.0\n",
      "...           ...        ...        ...       ...       ...       ...    ...\n",
      "233803  -8.526870   9.205021   7.646483 -1.475977 -1.290600 -0.301872    3.0\n",
      "233804  -1.152478   8.927959   9.071304 -2.094907  1.579927  0.227422    3.0\n",
      "233805  -2.547585   6.239240  12.170110  1.000152  0.790138 -0.569987    3.0\n",
      "233806   1.053528  13.871860   0.063724 -3.259116 -2.046253  0.696361    3.0\n",
      "233807  -5.458344   2.140917  10.254290  6.457951 -0.729170  1.101775    3.0\n",
      "\n",
      "[233808 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data_resampled = pd.concat([X_resampled, y_resampled], axis = 1)\n",
    "print(data_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-newark",
   "metadata": {},
   "source": [
    "# Training, Validation, Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creating training, validation, and test sets from a data frame.\n",
    "    \n",
    "    @param frame: dataframe passed in\n",
    "    @return training, validation, and test sets created from frame passed in\n",
    "\"\"\"\n",
    "def training_validation_test_split(frame):\n",
    "    frame_copy = frame.copy()\n",
    "    training_set = frame_copy.sample(frac = 0.75, random_state = 0)\n",
    "    val_temp = frame_copy.drop(training_set.index)\n",
    "    validation_set = val_temp.sample(frac = 0.5, random_state = 0)\n",
    "    test_set = val_temp.drop(validation_set.index)\n",
    "    return training_set, validation_set, test_set\n",
    "\n",
    "training, val, test = training_validation_test_split(data_resampled)\n",
    "\n",
    "print(\"Training:\\n\", training)\n",
    "print(\"Validation:\\n\", val)\n",
    "print(\"Test:\\n\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-electronics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
