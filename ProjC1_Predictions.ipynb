{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-simon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.9\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 880, got 864\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#!{sys.executable} -m pip install keras\n",
    "#!{sys.executable} -m pip install --ignore-installed --upgrade tensorflow==1.6.0\n",
    "#!{sys.executable} -m pip install keras==2.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-cedar",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "velvet-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Based on the training data given, we are able to extract 7 attributes:\n",
    "    1. x accelerometer measurement\n",
    "    2. y accelerometer measurement\n",
    "    3. z accelerometer measurement\n",
    "    4. x gyroscope measurement\n",
    "    5. y gyroscope measurement\n",
    "    6. z gyroscope measurement\n",
    "    7. time stamp for accelerometer and gyroscope measures\n",
    "    \n",
    "    We start by creating a dataframe using the csv files provided for readability.\n",
    "    \n",
    "    @param x_file: contains the xyz accelerometers and xyz gyroscope measures from the lower limb\n",
    "    @param x_time_file: contain the time stamps for the accelerometer and gyroscope measures\n",
    "    @return dataframe of 7 attributes mentioned\n",
    "\"\"\"\n",
    "def create_dataframe_X(x_file, x_time_file):\n",
    "    df1 = pd.read_csv(x_file, sep = ',', names = ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df2 = pd.read_csv(x_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We have both the labels and the time stamps for the labels. We create a dataframe from these for\n",
    "    readability.\n",
    "    \n",
    "    @param y_file: contain the labels: \n",
    "        (0) indicates standing or walking in solid ground, \n",
    "        (1) indicates going down the stairs, \n",
    "        (2) indicates going up the stairs, and \n",
    "        (3) indicates walking on grass\n",
    "    @param y_time_file: contain the time stamps for the labels\n",
    "    @return dataframe of labels and time stamps\n",
    "\"\"\" \n",
    "def create_dataframe_Y(y_file, y_time_file):\n",
    "    df1 = pd.read_csv(y_file, names = ['Label'])\n",
    "    df2 = pd.read_csv(y_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We take the outputs of create_dataframe_X and create_dataframe_Y. In order to combine both of these\n",
    "    dataframes, we need look at the time intervals present for when the labels were assigned. The goal is\n",
    "    to return a dataframe that now has an eighth column in addition to the seven columns from the dataframe\n",
    "    from create_dataframe_X. Additionally, we know that x_frame contains more values than y_frame. We want to\n",
    "    map these labels accordingly. In the end, we drop data points that have missing values.\n",
    "    \n",
    "    @param x_frame: dataframe from create_dataframe_X\n",
    "    @param y_frame: dataframe from create_dataframe_Y\n",
    "    @return dataframe with 8 columns (7 attributes and label)\n",
    "\"\"\"\n",
    "def combine_frames(x_frame, y_frame):\n",
    "    # Change each dataframe column to a list for iterations\n",
    "    labels = y_frame['Label'].tolist()\n",
    "    time_stamp_y = y_frame['Time stamp'].tolist()\n",
    "    time_stamp_x = x_frame['Time stamp'].tolist()\n",
    "    \n",
    "    labels_for_x = [] # Create empty list to gather corresponding labels for x_frame\n",
    "    count = 0\n",
    "    for i in range(0, len(time_stamp_y)):\n",
    "        while (time_stamp_x[count] <= time_stamp_y[i]) and (count <= len(time_stamp_x)):\n",
    "            labels_for_x.append(labels[i])\n",
    "            count += 1\n",
    "        continue\n",
    "    \n",
    "    # Concatenate the dataframes\n",
    "    label_df = pd.DataFrame(labels_for_x, columns = ['Label']) # Convert list back to data frame\n",
    "    combined_frame = pd.concat([x_frame, label_df], axis = 1)\n",
    "    \n",
    "    # Drop missing values at the end\n",
    "    combined_frame = combined_frame.dropna()\n",
    "    \n",
    "    # Drop 'Time stamp' column\n",
    "    combined_frame = combined_frame.drop(columns = ['Time stamp'])\n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "declared-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X_acc     Y_acc     Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "0      1.726654  9.619981  1.723327 -0.001997  0.067502  0.126057    0.0\n",
      "1      2.225759  9.493385  1.782374  0.008557  0.029333  0.073573    0.0\n",
      "2      2.010621  9.481603  1.770000 -0.004651  0.001009  0.062978    0.0\n",
      "3      1.614272  9.516440  1.798932  0.009519  0.024405  0.032554    0.0\n",
      "4      1.862582  9.353709  1.722649  0.007902  0.022794  0.020837    0.0\n",
      "...         ...       ...       ...       ...       ...       ...    ...\n",
      "70164  3.704972  8.586173  3.088743 -0.010505  0.009598 -0.004949    0.0\n",
      "70165  3.690854  8.759488  3.099146 -0.002501  0.001989  0.001526    0.0\n",
      "70166  3.939186  8.407883  3.049837  0.015672  0.011588  0.014313    0.0\n",
      "70167  3.762566  8.168921  3.062974  0.015675  0.007165  0.019624    0.0\n",
      "70168  3.729076  8.256303  3.034621 -0.005977  0.006976  0.006051    0.0\n",
      "\n",
      "[70169 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "x = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "y = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "combined = combine_frames(x, y)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stock-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generating data frames from training data.\n",
    "\"\"\"\n",
    "# Subject_001_01\n",
    "df_x_1_1 = create_dataframe_X('TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_01__x_time.csv')\n",
    "df_y_1_1 = create_dataframe_Y('TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_01__y_time.csv')\n",
    "frame_1_1 = combine_frames(df_x_1_1, df_y_1_1)\n",
    "\n",
    "# Subject_001_02\n",
    "df_x_1_2 = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "df_y_1_2 = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "frame_1_2 = combine_frames(df_x_1_2, df_y_1_2)\n",
    "\n",
    "# Subject_001_03\n",
    "df_x_1_3 = create_dataframe_X('TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_03__x_time.csv')\n",
    "df_y_1_3 = create_dataframe_Y('TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_03__y_time.csv')\n",
    "frame_1_3 = combine_frames(df_x_1_3, df_y_1_3)\n",
    "\n",
    "# Subject_001_04\n",
    "df_x_1_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_1_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_1_4 = combine_frames(df_x_1_4, df_y_1_4)\n",
    "\n",
    "# Subject_001_05\n",
    "df_x_1_5 = create_dataframe_X('TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_05__x_time.csv')\n",
    "df_y_1_5 = create_dataframe_Y('TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_05__y_time.csv')\n",
    "frame_1_5 = combine_frames(df_x_1_5, df_y_1_5)\n",
    "\n",
    "# Subject_001_06\n",
    "df_x_1_6 = create_dataframe_X('TrainingData/subject_001_06__x.csv', 'TrainingData/subject_001_06__x_time.csv')\n",
    "df_y_1_6 = create_dataframe_Y('TrainingData/subject_001_06__y.csv', 'TrainingData/subject_001_06__y_time.csv')\n",
    "frame_1_6 = combine_frames(df_x_1_6, df_y_1_6)\n",
    "\n",
    "# Subject_001_07\n",
    "df_x_1_7 = create_dataframe_X('TrainingData/subject_001_07__x.csv', 'TrainingData/subject_001_07__x_time.csv')\n",
    "df_y_1_7 = create_dataframe_Y('TrainingData/subject_001_07__y.csv', 'TrainingData/subject_001_07__y_time.csv')\n",
    "frame_1_7 = combine_frames(df_x_1_7, df_y_1_7)\n",
    "\n",
    "# Subject_001_08\n",
    "df_x_1_8 = create_dataframe_X('TrainingData/subject_001_08__x.csv', 'TrainingData/subject_001_08__x_time.csv')\n",
    "df_y_1_8 = create_dataframe_Y('TrainingData/subject_001_08__y.csv', 'TrainingData/subject_001_08__y_time.csv')\n",
    "frame_1_8 = combine_frames(df_x_1_8, df_y_1_8)\n",
    "\n",
    "# Subject_002_01\n",
    "df_x_2_1 = create_dataframe_X('TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_01__x_time.csv')\n",
    "df_y_2_1 = create_dataframe_Y('TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_01__y_time.csv')\n",
    "frame_2_1 = combine_frames(df_x_2_1, df_y_2_1)\n",
    "\n",
    "# Subject_002_02\n",
    "df_x_2_2 = create_dataframe_X('TrainingData/subject_002_02__x.csv', 'TrainingData/subject_002_02__x_time.csv')\n",
    "df_y_2_2 = create_dataframe_Y('TrainingData/subject_002_02__y.csv', 'TrainingData/subject_002_02__y_time.csv')\n",
    "frame_2_2 = combine_frames(df_x_2_2, df_y_2_2)\n",
    "\n",
    "# Subject_002_03\n",
    "df_x_2_3 = create_dataframe_X('TrainingData/subject_002_03__x.csv', 'TrainingData/subject_002_03__x_time.csv')\n",
    "df_y_2_3 = create_dataframe_Y('TrainingData/subject_002_03__y.csv', 'TrainingData/subject_002_03__y_time.csv')\n",
    "frame_2_3 = combine_frames(df_x_2_3, df_y_2_3)\n",
    "\n",
    "# Subject_002_04\n",
    "df_x_2_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_2_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_2_4 = combine_frames(df_x_2_4, df_y_2_4)\n",
    "\n",
    "# Subject_002_05\n",
    "df_x_2_5 = create_dataframe_X('TrainingData/subject_002_05__x.csv', 'TrainingData/subject_002_05__x_time.csv')\n",
    "df_y_2_5 = create_dataframe_Y('TrainingData/subject_002_05__y.csv', 'TrainingData/subject_002_05__y_time.csv')\n",
    "frame_2_5 = combine_frames(df_x_2_5, df_y_2_5)\n",
    "\n",
    "# Subject_003_01\n",
    "df_x_3_1 = create_dataframe_X('TrainingData/subject_003_01__x.csv', 'TrainingData/subject_003_01__x_time.csv')\n",
    "df_y_3_1 = create_dataframe_Y('TrainingData/subject_003_01__y.csv', 'TrainingData/subject_003_01__y_time.csv')\n",
    "frame_3_1 = combine_frames(df_x_3_1, df_y_3_1)\n",
    "\n",
    "# Subject_003_02\n",
    "df_x_3_2 = create_dataframe_X('TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_02__x_time.csv')\n",
    "df_y_3_2 = create_dataframe_Y('TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_02__y_time.csv')\n",
    "frame_3_2 = combine_frames(df_x_3_2, df_y_3_2)\n",
    "\n",
    "# Subject_003_03\n",
    "df_x_3_3 = create_dataframe_X('TrainingData/subject_003_03__x.csv', 'TrainingData/subject_003_03__x_time.csv')\n",
    "df_y_3_3 = create_dataframe_Y('TrainingData/subject_003_03__y.csv', 'TrainingData/subject_003_03__y_time.csv')\n",
    "frame_3_3 = combine_frames(df_x_3_3, df_y_3_3)\n",
    "\n",
    "# Subject_004_01\n",
    "df_x_4_1 = create_dataframe_X('TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_01__x_time.csv')\n",
    "df_y_4_1 = create_dataframe_Y('TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_01__y_time.csv')\n",
    "frame_4_1 = combine_frames(df_x_4_1, df_y_4_1)\n",
    "\n",
    "# Subject_004_02\n",
    "df_x_4_2 = create_dataframe_X('TrainingData/subject_004_02__x.csv', 'TrainingData/subject_004_02__x_time.csv')\n",
    "df_y_4_2 = create_dataframe_Y('TrainingData/subject_004_02__y.csv', 'TrainingData/subject_004_02__y_time.csv')\n",
    "frame_4_2 = combine_frames(df_x_4_2, df_y_4_2)\n",
    "\n",
    "# Subject_005_01\n",
    "df_x_5_1 = create_dataframe_X('TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_01__x_time.csv')\n",
    "df_y_5_1 = create_dataframe_Y('TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_01__y_time.csv')\n",
    "frame_5_1 = combine_frames(df_x_5_1, df_y_5_1)\n",
    "\n",
    "# Subject_005_02\n",
    "df_x_5_2 = create_dataframe_X('TrainingData/subject_005_02__x.csv', 'TrainingData/subject_005_02__x_time.csv')\n",
    "df_y_5_2 = create_dataframe_Y('TrainingData/subject_005_02__y.csv', 'TrainingData/subject_005_02__y_time.csv')\n",
    "frame_5_2 = combine_frames(df_x_5_2, df_y_5_2)\n",
    "\n",
    "# Subject_005_03\n",
    "df_x_5_3 = create_dataframe_X('TrainingData/subject_005_03__x.csv', 'TrainingData/subject_005_03__x_time.csv')\n",
    "df_y_5_3 = create_dataframe_Y('TrainingData/subject_005_03__y.csv', 'TrainingData/subject_005_03__y_time.csv')\n",
    "frame_5_3 = combine_frames(df_x_5_3, df_y_5_3)\n",
    "\n",
    "# Subject_006_01\n",
    "df_x_6_1 = create_dataframe_X('TrainingData/subject_006_01__x.csv', 'TrainingData/subject_006_01__x_time.csv')\n",
    "df_y_6_1 = create_dataframe_Y('TrainingData/subject_006_01__y.csv', 'TrainingData/subject_006_01__y_time.csv')\n",
    "frame_6_1 = combine_frames(df_x_6_1, df_y_6_1)\n",
    "\n",
    "# Subject_006_02\n",
    "df_x_6_2 = create_dataframe_X('TrainingData/subject_006_02__x.csv', 'TrainingData/subject_006_02__x_time.csv')\n",
    "df_y_6_2 = create_dataframe_Y('TrainingData/subject_006_02__y.csv', 'TrainingData/subject_006_02__y_time.csv')\n",
    "frame_6_2 = combine_frames(df_x_6_2, df_y_6_2)\n",
    "\n",
    "# Subject_006_03\n",
    "df_x_6_3 = create_dataframe_X('TrainingData/subject_006_03__x.csv', 'TrainingData/subject_006_03__x_time.csv')\n",
    "df_y_6_3 = create_dataframe_Y('TrainingData/subject_006_03__y.csv', 'TrainingData/subject_006_03__y_time.csv')\n",
    "frame_6_3 = combine_frames(df_x_6_3, df_y_6_3)\n",
    "\n",
    "# Subject_007_01\n",
    "df_x_7_1 = create_dataframe_X('TrainingData/subject_007_01__x.csv', 'TrainingData/subject_007_01__x_time.csv')\n",
    "df_y_7_1 = create_dataframe_Y('TrainingData/subject_007_01__y.csv', 'TrainingData/subject_007_01__y_time.csv')\n",
    "frame_7_1 = combine_frames(df_x_7_1, df_y_7_1)\n",
    "\n",
    "# Subject_007_02\n",
    "df_x_7_2 = create_dataframe_X('TrainingData/subject_007_02__x.csv', 'TrainingData/subject_007_02__x_time.csv')\n",
    "df_y_7_2 = create_dataframe_Y('TrainingData/subject_007_02__y.csv', 'TrainingData/subject_007_02__y_time.csv')\n",
    "frame_7_2 = combine_frames(df_x_7_2, df_y_7_2)\n",
    "\n",
    "# Subject_007_03\n",
    "df_x_7_3 = create_dataframe_X('TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_03__x_time.csv')\n",
    "df_y_7_3 = create_dataframe_Y('TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_03__y_time.csv')\n",
    "frame_7_3 = combine_frames(df_x_7_3, df_y_7_3)\n",
    "\n",
    "# Subject_007_04\n",
    "df_x_7_4 = create_dataframe_X('TrainingData/subject_007_04__x.csv', 'TrainingData/subject_007_04__x_time.csv')\n",
    "df_y_7_4 = create_dataframe_Y('TrainingData/subject_007_04__y.csv', 'TrainingData/subject_007_04__y_time.csv')\n",
    "frame_7_4 = combine_frames(df_x_7_4, df_y_7_4)\n",
    "\n",
    "# Subject_008_01\n",
    "df_x_8_1 = create_dataframe_X('TrainingData/subject_008_01__x.csv', 'TrainingData/subject_008_01__x_time.csv')\n",
    "df_y_8_1 = create_dataframe_Y('TrainingData/subject_008_01__y.csv', 'TrainingData/subject_008_01__y_time.csv')\n",
    "frame_8_1 = combine_frames(df_x_8_1, df_y_8_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "determined-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Combining all data frames and applying random undersampling.\n",
    "\"\"\"\n",
    "frame_list = [frame_1_1, frame_1_2, frame_1_3, frame_1_4, frame_1_5, frame_1_6, frame_1_7, frame_1_8,\n",
    "             frame_2_1, frame_2_2, frame_2_3, frame_2_4, frame_2_5,\n",
    "             frame_3_1, frame_3_2, frame_3_3,\n",
    "             frame_4_1, frame_4_2,\n",
    "             frame_5_1, frame_5_2, frame_5_3,\n",
    "             frame_6_1, frame_6_2, frame_6_3,\n",
    "             frame_7_1, frame_7_2, frame_7_3, frame_7_4,\n",
    "             frame_8_1]\n",
    "\n",
    "data = pd.concat(frame_list)\n",
    "\n",
    "# Create X and y\n",
    "X = data[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr']]\n",
    "y = data['Label']\n",
    "\n",
    "# Random undersampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# Combining resampled dataframes\n",
    "data_resampled = pd.concat([X_resampled, y_resampled], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-alfred",
   "metadata": {},
   "source": [
    "# Training, Validation, Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlled-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "99374   0.443280   2.371991  11.824440  5.485730 -2.511950 -0.597672    1.0\n",
      "54135  -2.832240   9.580369   4.016701 -0.882984 -1.398961 -0.539089    0.0\n",
      "147319  9.243908  -6.753744   5.495813  3.885012 -0.866962 -0.969324    2.0\n",
      "228455  6.076344   5.857147   7.815936 -1.373564 -0.952581 -0.321045    3.0\n",
      "213840  3.544153  -0.824973   7.932295  5.702078  2.552324  2.608943    3.0\n",
      "...          ...        ...        ...       ...       ...       ...    ...\n",
      "188023  7.543629   8.334155  12.425790 -4.262642  2.359509  1.674000    3.0\n",
      "126153  6.413435   4.418072  10.652710 -2.495509 -0.764505 -1.058842    2.0\n",
      "189727  4.970114  18.413130   1.925700 -4.149369 -1.792227 -0.524522    3.0\n",
      "3361    2.103770   0.991507  13.809880 -0.598645  0.030965 -0.142732    0.0\n",
      "82986   1.716331  17.573390   7.859266 -1.511394  0.912361 -0.011570    1.0\n",
      "\n",
      "[175356 rows x 7 columns]\n",
      "Validation:\n",
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "109065  1.340281  19.808290  18.130480 -4.541178 -1.831368  0.112899    1.0\n",
      "44364  -8.539748   4.582397  14.544850  6.889263  0.400195 -1.299423    0.0\n",
      "217260  3.371894   3.916124  13.343100  7.390979 -0.746827  0.277075    3.0\n",
      "63374  -2.138583   7.195535   2.580936 -1.638861 -0.209257  0.014403    1.0\n",
      "181513 -6.966177   8.878353   5.598353 -0.772569 -1.955664 -0.453347    3.0\n",
      "...          ...        ...        ...       ...       ...       ...    ...\n",
      "118975 -1.140397   7.653902   5.126128  0.615365 -0.779301  0.075206    2.0\n",
      "232323 -1.253060   7.831704   1.908522 -1.542577 -0.173245  0.169999    3.0\n",
      "112411  3.333670   3.381671   6.915709  1.356015  1.003957  0.422088    1.0\n",
      "84116  -2.759807   2.870911   5.639007  4.469371  3.492041  1.472650    1.0\n",
      "148788  1.847418   0.546901   5.934170  2.227376  0.792258  0.209967    2.0\n",
      "\n",
      "[29226 rows x 7 columns]\n",
      "Test:\n",
      "            X_acc      Y_acc      Z_acc     X_gyr     Y_gyr     Z_gyr  Label\n",
      "21      2.283801  14.918490  10.697530 -1.511189  3.630429  0.596639    0.0\n",
      "37      0.545069   8.791847   2.719765 -0.688419 -0.134145 -0.128697    0.0\n",
      "43      1.927812   9.448925   3.091286  0.125508 -0.272825 -0.027163    0.0\n",
      "46     -5.403930   7.875444   3.436919 -0.671170 -2.523009 -0.565114    0.0\n",
      "55      6.595331  10.085760   1.164397 -1.492771  0.562250  0.327917    0.0\n",
      "...          ...        ...        ...       ...       ...       ...    ...\n",
      "233752  7.481142   1.636054  10.729130 -3.136037  1.572726  0.349721    3.0\n",
      "233756 -4.145525   9.276828   7.439458 -3.423434  1.573703  1.258922    3.0\n",
      "233761  2.138197  10.914870  -0.846361 -1.624774  0.072213 -0.306671    3.0\n",
      "233783  3.553030  11.240610   2.777171 -2.718525 -0.423632 -0.296170    3.0\n",
      "233792  3.911373   1.110181  10.930240  6.532366  2.768746  0.224613    3.0\n",
      "\n",
      "[29226 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Creating training, validation, and test sets from a data frame.\n",
    "    \n",
    "    @param frame: dataframe passed in\n",
    "    @return training, validation, and test sets created from frame passed in\n",
    "\"\"\"\n",
    "def training_validation_test_split(frame):\n",
    "    frame_copy = frame.copy()\n",
    "    training_set = frame_copy.sample(frac = 0.75, random_state = 0)\n",
    "    val_temp = frame_copy.drop(training_set.index)\n",
    "    validation_set = val_temp.sample(frac = 0.5, random_state = 0)\n",
    "    test_set = val_temp.drop(validation_set.index)\n",
    "    return training_set, validation_set, test_set\n",
    "\n",
    "training, val, test = training_validation_test_split(data_resampled)\n",
    "\n",
    "print(\"Training:\\n\", training)\n",
    "print(\"Validation:\\n\", val)\n",
    "print(\"Test:\\n\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "historical-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training X and y\n",
    "training_X = training[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr']]\n",
    "training_y = training['Label']\n",
    "\n",
    "# Create validation X and y\n",
    "val_X = val[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr']]\n",
    "val_y = val['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exempt-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "encoder_training = LabelBinarizer()\n",
    "encoder_val = LabelBinarizer()\n",
    "encoder_training.fit(training_y)\n",
    "training_y_encoded = encoder_training.transform(training_y)\n",
    "encoder_val.fit(val_y)\n",
    "val_y_encoded = encoder_val.transform(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-renewal",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "Our dataset consists of 6 numerical input variables. There are four output classes, which are one-hot encoded as follows:\n",
    "\n",
    "* 0: [1 0 0 0]\n",
    "* 1: [0 1 0 0]\n",
    "* 2: [0 0 1 0]\n",
    "* 3: [0 0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 175356 samples, validate on 29226 samples\n",
      "Epoch 1/100\n",
      "175356/175356 [==============================] - 16s 92us/step - loss: 1.3649 - acc: 0.3249 - val_loss: 1.2773 - val_acc: 0.3600\n",
      "Epoch 2/100\n",
      "175356/175356 [==============================] - 15s 88us/step - loss: 1.2472 - acc: 0.3755 - val_loss: 1.2281 - val_acc: 0.3824\n",
      "Epoch 3/100\n",
      "175356/175356 [==============================] - 16s 91us/step - loss: 1.2071 - acc: 0.3890 - val_loss: 1.1958 - val_acc: 0.3995\n",
      "Epoch 4/100\n",
      "175356/175356 [==============================] - 16s 89us/step - loss: 1.1794 - acc: 0.3888 - val_loss: 1.1702 - val_acc: 0.3878\n",
      "Epoch 5/100\n",
      "175356/175356 [==============================] - 20s 114us/step - loss: 1.1636 - acc: 0.3916 - val_loss: 1.1575 - val_acc: 0.3925\n",
      "Epoch 6/100\n",
      "175356/175356 [==============================] - 18s 102us/step - loss: 1.1510 - acc: 0.4014 - val_loss: 1.1454 - val_acc: 0.4080\n",
      "Epoch 7/100\n",
      "175356/175356 [==============================] - 18s 102us/step - loss: 1.1416 - acc: 0.4083 - val_loss: 1.1364 - val_acc: 0.4118\n",
      "Epoch 8/100\n",
      "175356/175356 [==============================] - 19s 109us/step - loss: 1.1373 - acc: 0.4104 - val_loss: 1.1376 - val_acc: 0.4164\n",
      "Epoch 9/100\n",
      "175356/175356 [==============================] - 16s 93us/step - loss: 1.1351 - acc: 0.4116 - val_loss: 1.1349 - val_acc: 0.4156\n",
      "Epoch 10/100\n",
      "175356/175356 [==============================] - 17s 94us/step - loss: 1.1338 - acc: 0.4112 - val_loss: 1.1340 - val_acc: 0.4129\n",
      "Epoch 11/100\n",
      "175356/175356 [==============================] - 18s 101us/step - loss: 1.1327 - acc: 0.4112 - val_loss: 1.1341 - val_acc: 0.4177\n",
      "Epoch 12/100\n",
      "175356/175356 [==============================] - 17s 99us/step - loss: 1.1319 - acc: 0.4112 - val_loss: 1.1316 - val_acc: 0.4147\n",
      "Epoch 13/100\n",
      "175356/175356 [==============================] - 19s 110us/step - loss: 1.1313 - acc: 0.4126 - val_loss: 1.1310 - val_acc: 0.4195\n",
      "Epoch 14/100\n",
      "175356/175356 [==============================] - 17s 100us/step - loss: 1.1305 - acc: 0.4126 - val_loss: 1.1287 - val_acc: 0.4163\n",
      "Epoch 15/100\n",
      "175356/175356 [==============================] - 17s 97us/step - loss: 1.1300 - acc: 0.4123 - val_loss: 1.1315 - val_acc: 0.4126\n",
      "Epoch 16/100\n",
      "175356/175356 [==============================] - 17s 98us/step - loss: 1.1293 - acc: 0.4143 - val_loss: 1.1322 - val_acc: 0.4156\n",
      "Epoch 17/100\n",
      "175356/175356 [==============================] - 18s 105us/step - loss: 1.1287 - acc: 0.4140 - val_loss: 1.1279 - val_acc: 0.4196\n",
      "Epoch 18/100\n",
      "175356/175356 [==============================] - 18s 101us/step - loss: 1.1277 - acc: 0.4141 - val_loss: 1.1258 - val_acc: 0.4206\n",
      "Epoch 19/100\n",
      "175356/175356 [==============================] - 19s 111us/step - loss: 1.1271 - acc: 0.4150 - val_loss: 1.1242 - val_acc: 0.4210\n",
      "Epoch 20/100\n",
      "175356/175356 [==============================] - 23s 130us/step - loss: 1.1265 - acc: 0.4166 - val_loss: 1.1250 - val_acc: 0.4181\n",
      "Epoch 21/100\n",
      "175356/175356 [==============================] - 19s 107us/step - loss: 1.1262 - acc: 0.4157 - val_loss: 1.1274 - val_acc: 0.4181\n",
      "Epoch 22/100\n",
      "175356/175356 [==============================] - 38s 216us/step - loss: 1.1257 - acc: 0.4156 - val_loss: 1.1248 - val_acc: 0.4176\n",
      "Epoch 23/100\n",
      "175356/175356 [==============================] - 33s 187us/step - loss: 1.1255 - acc: 0.4163 - val_loss: 1.1235 - val_acc: 0.4212\n",
      "Epoch 24/100\n",
      "175356/175356 [==============================] - 26s 149us/step - loss: 1.1252 - acc: 0.4162 - val_loss: 1.1272 - val_acc: 0.4158\n",
      "Epoch 25/100\n",
      "175356/175356 [==============================] - 26s 149us/step - loss: 1.1250 - acc: 0.4158 - val_loss: 1.1254 - val_acc: 0.4204\n",
      "Epoch 26/100\n",
      "175356/175356 [==============================] - 20s 116us/step - loss: 1.1248 - acc: 0.4150 - val_loss: 1.1238 - val_acc: 0.4194\n",
      "Epoch 27/100\n",
      "175356/175356 [==============================] - 18s 102us/step - loss: 1.1245 - acc: 0.4158 - val_loss: 1.1264 - val_acc: 0.4178\n",
      "Epoch 28/100\n",
      "175356/175356 [==============================] - 18s 103us/step - loss: 1.1243 - acc: 0.4149 - val_loss: 1.1245 - val_acc: 0.4195\n",
      "Epoch 29/100\n",
      "175356/175356 [==============================] - 24s 136us/step - loss: 1.1238 - acc: 0.4152 - val_loss: 1.1224 - val_acc: 0.4191\n",
      "Epoch 30/100\n",
      "175356/175356 [==============================] - 26s 147us/step - loss: 1.1237 - acc: 0.4144 - val_loss: 1.1220 - val_acc: 0.4214\n",
      "Epoch 31/100\n",
      "175356/175356 [==============================] - 38s 216us/step - loss: 1.1233 - acc: 0.4144 - val_loss: 1.1238 - val_acc: 0.4203\n",
      "Epoch 32/100\n",
      "175356/175356 [==============================] - 22s 124us/step - loss: 1.1232 - acc: 0.4141 - val_loss: 1.1231 - val_acc: 0.4182\n",
      "Epoch 33/100\n",
      "175356/175356 [==============================] - 25s 144us/step - loss: 1.1229 - acc: 0.4147 - val_loss: 1.1236 - val_acc: 0.4185\n",
      "Epoch 34/100\n",
      " 23680/175356 [===>..........................] - ETA: 17s - loss: 1.1237 - acc: 0.4153"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    @param training_X: training inputs\n",
    "    @param training_y: one-hot encoded training labels\n",
    "    @param val_X: validation inputs\n",
    "    @param val_y: one-hot encoded validation labels\n",
    "\"\"\"\n",
    "def model(training_X, training_y, val_X, val_y, epochs):\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim = training_X.shape[1], activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(4, activation = 'sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "    # Fit the Keras model on the dataset\n",
    "    model.fit(training_X, training_y, epochs = epochs, validation_data = (val_X, val_y), batch_size = 20, verbose = 1)\n",
    "    \n",
    "model(training_X, training_y_encoded, val_X, val_y_encoded, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-monday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
