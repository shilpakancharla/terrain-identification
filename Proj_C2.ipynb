{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "confident-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "from tensorflow.python.keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.utils import to_categorical\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-purple",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "anonymous-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Based on the training data given, we are able to extract 7 attributes:\n",
    "    1. x accelerometer measurement\n",
    "    2. y accelerometer measurement\n",
    "    3. z accelerometer measurement\n",
    "    4. x gyroscope measurement\n",
    "    5. y gyroscope measurement\n",
    "    6. z gyroscope measurement\n",
    "    7. time stamp for accelerometer and gyroscope measures\n",
    "    \n",
    "    We start by creating a dataframe using the csv files provided for readability.\n",
    "    \n",
    "    @param x_file: contains the xyz accelerometers and xyz gyroscope measures from the lower limb\n",
    "    @param x_time_file: contain the time stamps for the accelerometer and gyroscope measures\n",
    "    @return dataframe of 7 attributes mentioned\n",
    "\"\"\"\n",
    "def create_dataframe_X(x_file, x_time_file):\n",
    "    df1 = pd.read_csv(x_file, sep = ',', names = ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df1 = scale_data(df1, ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df2 = pd.read_csv(x_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "    Scale the values of X to make it robust to outliers.\n",
    "    \n",
    "    @param df: input dataframe\n",
    "    @param columns: columns to scale\n",
    "    @return scaled dataframe\n",
    "\"\"\"\n",
    "def scale_data(df, columns):\n",
    "    scaler = RobustScaler()\n",
    "    scaler = scaler.fit(df[columns])\n",
    "    df.loc[:, columns] = scaler.transform(df[columns])\n",
    "    return df\n",
    "    \n",
    "\"\"\"\n",
    "    We have both the labels and the time stamps for the labels. We create a dataframe from these for\n",
    "    readability.\n",
    "    \n",
    "    @param y_file: contain the labels: \n",
    "        (0) indicates standing or walking in solid ground, \n",
    "        (1) indicates going down the stairs, \n",
    "        (2) indicates going up the stairs, and \n",
    "        (3) indicates walking on grass\n",
    "    @param y_time_file: contain the time stamps for the labels\n",
    "    @return dataframe of labels and time stamps\n",
    "\"\"\" \n",
    "def create_dataframe_Y(y_file, y_time_file):\n",
    "    df1 = pd.read_csv(y_file, names = ['Label'])\n",
    "    df2 = pd.read_csv(y_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We take the outputs of create_dataframe_X and create_dataframe_Y. In order to combine both of these\n",
    "    dataframes, we need look at the time intervals present for when the labels were assigned. We down-sample\n",
    "    the X to the shape of the y.\n",
    "    \n",
    "    @param x_frame: dataframe from create_dataframe_X\n",
    "    @param y_frame: dataframe from create_dataframe_Y\n",
    "    @return dataframe with 9 columns (8 attributes and 1 label)\n",
    "\"\"\"\n",
    "def combine_frames(x_frame, y_frame):\n",
    "    # Change each dataframe column to a list for iterations\n",
    "    time_stamp_y = y_frame['Time stamp'].tolist()\n",
    "    time_stamp_x = x_frame['Time stamp'].tolist()\n",
    "    \n",
    "    x_range = [] # Empty list to append data points to\n",
    "    x_random_row = 0 # Initializing variable to hold randomly selected row instance\n",
    "    refs = []\n",
    "    count = 0\n",
    "    for i in range(0, len(time_stamp_y)):\n",
    "        while (time_stamp_x[count] <= time_stamp_y[i]) and (count <= len(time_stamp_x)):\n",
    "            x_range.append(time_stamp_x.index(time_stamp_x[count]))\n",
    "            count += 1\n",
    "        x_random_row = random.choice(x_range) # Pick a random value\n",
    "        refs.append(x_random_row) # Keep record of selected rows\n",
    "        x_range.clear() # Clear the cache\n",
    "        continue\n",
    "    \n",
    "    # Create a new dataframe based on the refs collected - should be roughly the same length as the y_frame\n",
    "    entries = []\n",
    "    for item in refs:\n",
    "        entry = x_frame.iloc[item]\n",
    "        entries.append(entry)\n",
    "    \n",
    "    found_df = pd.concat(entries, axis = 1)\n",
    "    found_df = found_df.transpose()\n",
    "    \n",
    "    # Combine found_df with y_frame for downsampling\n",
    "    found_df = found_df.reset_index()\n",
    "    found_df = found_df.drop(['index'], axis = 1)\n",
    "    found_df = found_df.drop(['Time stamp'], axis = 1)\n",
    "    combined_frame = pd.concat([found_df, y_frame], axis = 1)\n",
    "    return combined_frame\n",
    "\n",
    "\"\"\"\n",
    "    Takes in the sequential X and y and creates windows of time-series data.\n",
    "    \n",
    "    @param X: input data\n",
    "    @param y: label data\n",
    "    @param time_steps: determines size of window\n",
    "    @param step: incremental value that window will slide over\n",
    "    @return time series of X and y data\n",
    "\"\"\"\n",
    "def mode_labels(X, y, time_steps, step):\n",
    "    X_values = []\n",
    "    y_values = []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        value = X.iloc[i:(i + time_steps)].values\n",
    "        labels = y.iloc[i: (i + time_steps)]\n",
    "        X_values.append(value)\n",
    "        y_values.append(stats.mode(labels)[0][0])\n",
    "    return np.array(X_values), np.array(y_values).reshape(-1, 1)\n",
    "\n",
    "\"\"\"\n",
    "    Generating data frames from training data.\n",
    "    \n",
    "    @param X_file: list of input X files\n",
    "    @param X_t_file: list of input X_time files\n",
    "    @param y_file: list of input y files\n",
    "    @param y_t file: list of y_time files\n",
    "    @return stacked window of instances across all training files, stack window of labels across all label files\n",
    "\"\"\"\n",
    "def generate_data(X_file, X_t_file, y_file, y_t_file):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    for item_X, item_X_t, item_y, item_y_t in zip(X_file, X_t_file, y_file, y_t_file):\n",
    "        df_x = create_dataframe_X(item_X, item_X_t)\n",
    "        df_y = create_dataframe_Y(item_y, item_y_t)\n",
    "        combined_frame = combine_frames(df_x, df_y)\n",
    "        X_temp = combined_frame[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr', 'Time stamp']]\n",
    "        y_temp = combined_frame['Label']\n",
    "        X, y = mode_labels(X_temp, y_temp, 30, 1)\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    return np.concatenate(all_X), np.concatenate(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of training X_files\n",
    "X_files = ['TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_02__x.csv', \n",
    "           'TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_04__x.csv',\n",
    "           'TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_06__x.csv',\n",
    "           'TrainingData/subject_001_07__x.csv', 'TrainingData/subject_001_08__x.csv',\n",
    "           'TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_02__x.csv',\n",
    "           'TrainingData/subject_002_03__x.csv', 'TrainingData/subject_001_04__x.csv',\n",
    "           'TrainingData/subject_002_05__x.csv', 'TrainingData/subject_003_01__x.csv',\n",
    "           'TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_03__x.csv',\n",
    "           'TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_02__x.csv',\n",
    "           'TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_02__x.csv',\n",
    "           'TrainingData/subject_005_03__x.csv', 'TrainingData/subject_006_01__x.csv',\n",
    "           'TrainingData/subject_006_02__x.csv', 'TrainingData/subject_006_03__x.csv',\n",
    "           'TrainingData/subject_007_01__x.csv', 'TrainingData/subject_007_02__x.csv',\n",
    "           'TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_04__x.csv',\n",
    "           'TrainingData/subject_008_01__x.csv']\n",
    "\n",
    "# List of training X_t_files\n",
    "X_t_files = ['TrainingData/subject_001_01__x_time.csv', 'TrainingData/subject_001_02__x_time.csv', \n",
    "             'TrainingData/subject_001_03__x_time.csv', 'TrainingData/subject_001_04__x_time.csv',\n",
    "             'TrainingData/subject_001_05__x_time.csv', 'TrainingData/subject_001_06__x_time.csv',\n",
    "             'TrainingData/subject_001_07__x_time.csv', 'TrainingData/subject_001_08__x_time.csv',\n",
    "             'TrainingData/subject_002_01__x_time.csv', 'TrainingData/subject_002_02__x_time.csv',\n",
    "             'TrainingData/subject_002_03__x_time.csv', 'TrainingData/subject_001_04__x_time.csv',\n",
    "             'TrainingData/subject_002_05__x_time.csv', 'TrainingData/subject_003_01__x_time.csv',\n",
    "             'TrainingData/subject_003_02__x_time.csv', 'TrainingData/subject_003_03__x_time.csv',\n",
    "             'TrainingData/subject_004_01__x_time.csv', 'TrainingData/subject_004_02__x_time.csv',\n",
    "             'TrainingData/subject_005_01__x_time.csv', 'TrainingData/subject_005_02__x_time.csv',\n",
    "             'TrainingData/subject_005_03__x_time.csv', 'TrainingData/subject_006_01__x_time.csv',\n",
    "             'TrainingData/subject_006_02__x_time.csv', 'TrainingData/subject_006_03__x_time.csv',\n",
    "             'TrainingData/subject_007_01__x_time.csv', 'TrainingData/subject_007_02__x_time.csv',\n",
    "             'TrainingData/subject_007_03__x_time.csv', 'TrainingData/subject_007_04__x_time.csv',\n",
    "             'TrainingData/subject_008_01__x_time.csv']\n",
    "\n",
    "# List of training y_files\n",
    "y_files = ['TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_02__y.csv', \n",
    "           'TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_04__y.csv',\n",
    "           'TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_06__y.csv',\n",
    "           'TrainingData/subject_001_07__y.csv', 'TrainingData/subject_001_08__y.csv',\n",
    "           'TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_02__y.csv',\n",
    "           'TrainingData/subject_002_03__y.csv', 'TrainingData/subject_001_04__y.csv',\n",
    "           'TrainingData/subject_002_05__y.csv', 'TrainingData/subject_003_01__y.csv',\n",
    "           'TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_03__y.csv',\n",
    "           'TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_02__y.csv',\n",
    "           'TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_02__y.csv',\n",
    "           'TrainingData/subject_005_03__y.csv', 'TrainingData/subject_006_01__y.csv',\n",
    "           'TrainingData/subject_006_02__y.csv', 'TrainingData/subject_006_03__y.csv',\n",
    "           'TrainingData/subject_007_01__y.csv', 'TrainingData/subject_007_02__y.csv',\n",
    "           'TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_04__y.csv',\n",
    "           'TrainingData/subject_008_01__y.csv']\n",
    "\n",
    "# List of training y_t_files\n",
    "y_t_files = ['TrainingData/subject_001_01__y_time.csv', 'TrainingData/subject_001_02__y_time.csv', \n",
    "             'TrainingData/subject_001_03__y_time.csv', 'TrainingData/subject_001_04__y_time.csv',\n",
    "             'TrainingData/subject_001_05__y_time.csv', 'TrainingData/subject_001_06__y_time.csv',\n",
    "             'TrainingData/subject_001_07__y_time.csv', 'TrainingData/subject_001_08__y_time.csv',\n",
    "             'TrainingData/subject_002_01__y_time.csv', 'TrainingData/subject_002_02__y_time.csv',\n",
    "             'TrainingData/subject_002_03__y_time.csv', 'TrainingData/subject_001_04__y_time.csv',\n",
    "             'TrainingData/subject_002_05__y_time.csv', 'TrainingData/subject_003_01__y_time.csv',\n",
    "             'TrainingData/subject_003_02__y_time.csv', 'TrainingData/subject_003_03__y_time.csv',\n",
    "             'TrainingData/subject_004_01__y_time.csv', 'TrainingData/subject_004_02__y_time.csv',\n",
    "             'TrainingData/subject_005_01__y_time.csv', 'TrainingData/subject_005_02__y_time.csv',\n",
    "             'TrainingData/subject_005_03__y_time.csv', 'TrainingData/subject_006_01__y_time.csv',\n",
    "             'TrainingData/subject_006_02__y_time.csv', 'TrainingData/subject_006_03__y_time.csv',\n",
    "             'TrainingData/subject_007_01__y_time.csv', 'TrainingData/subject_007_02__y_time.csv',\n",
    "             'TrainingData/subject_007_03__y_time.csv', 'TrainingData/subject_007_04__y_time.csv',\n",
    "             'TrainingData/subject_008_01__y_time.csv']\n",
    "\n",
    "X, y = generate_data(X_files, X_t_files, y_files, y_t_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legislative-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing random undersampling on the data\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "data_resampled = pd.concat([X_resampled, y_resampled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afraid-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creating training and validation sets from the dataframe.\n",
    "    \n",
    "    @param frame: dataframe passed in\n",
    "    @return training and validation sets created from frame passed in\n",
    "\"\"\"\n",
    "def training_validation_split(frame):\n",
    "    frame_copy = frame.copy()\n",
    "    training_set = frame_copy.sample(frac = 0.70, random_state = 0)\n",
    "    validation_set = frame_copy.drop(training_set.index)\n",
    "    return training_set, validation_set\n",
    "\n",
    "training, val = training_validation_split(data_resampled)\n",
    "training_X = training[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr', 'Time stamp']]\n",
    "training_X = np.expand_dims(training_X, axis = 1)\n",
    "training_y = training['Label']\n",
    "training_y_encoded = to_categorical(training_y) # One-hot encoding\n",
    "val_X = val[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr', 'Time stamp']]\n",
    "val_X = np.expand_dims(val_X, axis = 1)\n",
    "val_y = val['Label']\n",
    "val_y_encoded = to_categorical(val_y) # One-hot encoding\n",
    "\n",
    "# 40916 timesteps, 7 features, 4 outputs\n",
    "n_timesteps, n_features, n_outputs = training_X.shape[1], training_X.shape[2], training_y_encoded.shape[1]\n",
    "\n",
    "def define_LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape = (n_timesteps, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(n_outputs, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "def evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded):\n",
    "    verbose, epochs, batch_size = 1, 15, 64\n",
    "    model = define_LSTM_model()\n",
    "    # Fit network\n",
    "    model.fit(training_X, training_y_encoded, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "    # Evaluate model\n",
    "    _, accuracy = model.evaluate(val_X, val_y_encoded, batch_size = batch_size, verbose = verbose)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
