{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confident-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-purple",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "anonymous-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Based on the training data given, we are able to extract 7 attributes:\n",
    "    1. x accelerometer measurement\n",
    "    2. y accelerometer measurement\n",
    "    3. z accelerometer measurement\n",
    "    4. x gyroscope measurement\n",
    "    5. y gyroscope measurement\n",
    "    6. z gyroscope measurement\n",
    "    7. time stamp for accelerometer and gyroscope measures\n",
    "    \n",
    "    We start by creating a dataframe using the csv files provided for readability.\n",
    "    \n",
    "    @param x_file: contains the xyz accelerometers and xyz gyroscope measures from the lower limb\n",
    "    @param x_time_file: contain the time stamps for the accelerometer and gyroscope measures\n",
    "    @return dataframe of 7 attributes mentioned\n",
    "\"\"\"\n",
    "def create_dataframe_X(x_file, x_time_file):\n",
    "    df1 = pd.read_csv(x_file, sep = ',', names = ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df2 = pd.read_csv(x_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We have both the labels and the time stamps for the labels. We create a dataframe from these for\n",
    "    readability.\n",
    "    \n",
    "    @param y_file: contain the labels: \n",
    "        (0) indicates standing or walking in solid ground, \n",
    "        (1) indicates going down the stairs, \n",
    "        (2) indicates going up the stairs, and \n",
    "        (3) indicates walking on grass\n",
    "    @param y_time_file: contain the time stamps for the labels\n",
    "    @return dataframe of labels and time stamps\n",
    "\"\"\" \n",
    "def create_dataframe_Y(y_file, y_time_file):\n",
    "    df1 = pd.read_csv(y_file, names = ['Label'])\n",
    "    df2 = pd.read_csv(y_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We take the outputs of create_dataframe_X and create_dataframe_Y. In order to combine both of these\n",
    "    dataframes, we need look at the time intervals present for when the labels were assigned. We down-sample\n",
    "    the X to the shape of the y.\n",
    "    \n",
    "    @param x_frame: dataframe from create_dataframe_X\n",
    "    @param y_frame: dataframe from create_dataframe_Y\n",
    "    @return dataframe with 9 columns (8 attributes and 1 label)\n",
    "\"\"\"\n",
    "def combine_frames(x_frame, y_frame):\n",
    "    # Change each dataframe column to a list for iterations\n",
    "    time_stamp_y = y_frame['Time stamp'].tolist()\n",
    "    time_stamp_x = x_frame['Time stamp'].tolist()\n",
    "    \n",
    "    x_range = [] # Empty list to append data points to\n",
    "    x_random_row = 0 # Initializing variable to hold randomly selected row instance\n",
    "    refs = []\n",
    "    count = 0\n",
    "    for i in range(0, len(time_stamp_y)):\n",
    "        while (time_stamp_x[count] <= time_stamp_y[i]) and (count <= len(time_stamp_x)):\n",
    "            x_range.append(time_stamp_x.index(time_stamp_x[count]))\n",
    "            count += 1\n",
    "        x_random_row = random.choice(x_range) # Pick a random value\n",
    "        refs.append(x_random_row) # Keep record of selected rows\n",
    "        x_range.clear() # Clear the cache\n",
    "        continue\n",
    "    \n",
    "    # Create a new dataframe based on the refs collected - should be roughly the same length as the y_frame\n",
    "    entries = []\n",
    "    for item in refs:\n",
    "        entry = x_frame.iloc[item]\n",
    "        entries.append(entry)\n",
    "    \n",
    "    found_df = pd.concat(entries, axis = 1)\n",
    "    found_df = found_df.transpose()\n",
    "    \n",
    "    # Combine found_df with y_frame for downsampling\n",
    "    found_df = found_df.reset_index()\n",
    "    found_df = found_df.drop(['index'], axis = 1)\n",
    "    found_df = found_df.drop(['Time stamp'], axis = 1)\n",
    "    combined_frame = pd.concat([found_df, y_frame], axis = 1)\n",
    "    \n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposed-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generating data frames from training data.\n",
    "\"\"\"\n",
    "# Subject_001_01\n",
    "df_x_1_1 = create_dataframe_X('TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_01__x_time.csv')\n",
    "df_y_1_1 = create_dataframe_Y('TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_01__y_time.csv')\n",
    "frame_1_1 = combine_frames(df_x_1_1, df_y_1_1)\n",
    "\n",
    "# Subject_001_02\n",
    "df_x_1_2 = create_dataframe_X('TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_02__x_time.csv')\n",
    "df_y_1_2 = create_dataframe_Y('TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_02__y_time.csv')\n",
    "frame_1_2 = combine_frames(df_x_1_2, df_y_1_2)\n",
    "\n",
    "# Subject_001_03\n",
    "df_x_1_3 = create_dataframe_X('TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_03__x_time.csv')\n",
    "df_y_1_3 = create_dataframe_Y('TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_03__y_time.csv')\n",
    "frame_1_3 = combine_frames(df_x_1_3, df_y_1_3)\n",
    "\n",
    "# Subject_001_04\n",
    "df_x_1_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_1_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_1_4 = combine_frames(df_x_1_4, df_y_1_4)\n",
    "\n",
    "# Subject_001_05\n",
    "df_x_1_5 = create_dataframe_X('TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_05__x_time.csv')\n",
    "df_y_1_5 = create_dataframe_Y('TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_05__y_time.csv')\n",
    "frame_1_5 = combine_frames(df_x_1_5, df_y_1_5)\n",
    "\n",
    "# Subject_001_06\n",
    "df_x_1_6 = create_dataframe_X('TrainingData/subject_001_06__x.csv', 'TrainingData/subject_001_06__x_time.csv')\n",
    "df_y_1_6 = create_dataframe_Y('TrainingData/subject_001_06__y.csv', 'TrainingData/subject_001_06__y_time.csv')\n",
    "frame_1_6 = combine_frames(df_x_1_6, df_y_1_6)\n",
    "\n",
    "# Subject_001_07\n",
    "df_x_1_7 = create_dataframe_X('TrainingData/subject_001_07__x.csv', 'TrainingData/subject_001_07__x_time.csv')\n",
    "df_y_1_7 = create_dataframe_Y('TrainingData/subject_001_07__y.csv', 'TrainingData/subject_001_07__y_time.csv')\n",
    "frame_1_7 = combine_frames(df_x_1_7, df_y_1_7)\n",
    "\n",
    "# Subject_001_08\n",
    "df_x_1_8 = create_dataframe_X('TrainingData/subject_001_08__x.csv', 'TrainingData/subject_001_08__x_time.csv')\n",
    "df_y_1_8 = create_dataframe_Y('TrainingData/subject_001_08__y.csv', 'TrainingData/subject_001_08__y_time.csv')\n",
    "frame_1_8 = combine_frames(df_x_1_8, df_y_1_8)\n",
    "\n",
    "# Subject_002_01\n",
    "df_x_2_1 = create_dataframe_X('TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_01__x_time.csv')\n",
    "df_y_2_1 = create_dataframe_Y('TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_01__y_time.csv')\n",
    "frame_2_1 = combine_frames(df_x_2_1, df_y_2_1)\n",
    "\n",
    "# Subject_002_02\n",
    "df_x_2_2 = create_dataframe_X('TrainingData/subject_002_02__x.csv', 'TrainingData/subject_002_02__x_time.csv')\n",
    "df_y_2_2 = create_dataframe_Y('TrainingData/subject_002_02__y.csv', 'TrainingData/subject_002_02__y_time.csv')\n",
    "frame_2_2 = combine_frames(df_x_2_2, df_y_2_2)\n",
    "\n",
    "# Subject_002_03\n",
    "df_x_2_3 = create_dataframe_X('TrainingData/subject_002_03__x.csv', 'TrainingData/subject_002_03__x_time.csv')\n",
    "df_y_2_3 = create_dataframe_Y('TrainingData/subject_002_03__y.csv', 'TrainingData/subject_002_03__y_time.csv')\n",
    "frame_2_3 = combine_frames(df_x_2_3, df_y_2_3)\n",
    "\n",
    "# Subject_002_04\n",
    "df_x_2_4 = create_dataframe_X('TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_04__x_time.csv')\n",
    "df_y_2_4 = create_dataframe_Y('TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_04__y_time.csv')\n",
    "frame_2_4 = combine_frames(df_x_2_4, df_y_2_4)\n",
    "\n",
    "# Subject_002_05\n",
    "df_x_2_5 = create_dataframe_X('TrainingData/subject_002_05__x.csv', 'TrainingData/subject_002_05__x_time.csv')\n",
    "df_y_2_5 = create_dataframe_Y('TrainingData/subject_002_05__y.csv', 'TrainingData/subject_002_05__y_time.csv')\n",
    "frame_2_5 = combine_frames(df_x_2_5, df_y_2_5)\n",
    "\n",
    "# Subject_003_01\n",
    "df_x_3_1 = create_dataframe_X('TrainingData/subject_003_01__x.csv', 'TrainingData/subject_003_01__x_time.csv')\n",
    "df_y_3_1 = create_dataframe_Y('TrainingData/subject_003_01__y.csv', 'TrainingData/subject_003_01__y_time.csv')\n",
    "frame_3_1 = combine_frames(df_x_3_1, df_y_3_1)\n",
    "\n",
    "# Subject_003_02\n",
    "df_x_3_2 = create_dataframe_X('TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_02__x_time.csv')\n",
    "df_y_3_2 = create_dataframe_Y('TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_02__y_time.csv')\n",
    "frame_3_2 = combine_frames(df_x_3_2, df_y_3_2)\n",
    "\n",
    "# Subject_003_03\n",
    "df_x_3_3 = create_dataframe_X('TrainingData/subject_003_03__x.csv', 'TrainingData/subject_003_03__x_time.csv')\n",
    "df_y_3_3 = create_dataframe_Y('TrainingData/subject_003_03__y.csv', 'TrainingData/subject_003_03__y_time.csv')\n",
    "frame_3_3 = combine_frames(df_x_3_3, df_y_3_3)\n",
    "\n",
    "# Subject_004_01\n",
    "df_x_4_1 = create_dataframe_X('TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_01__x_time.csv')\n",
    "df_y_4_1 = create_dataframe_Y('TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_01__y_time.csv')\n",
    "frame_4_1 = combine_frames(df_x_4_1, df_y_4_1)\n",
    "\n",
    "# Subject_004_02\n",
    "df_x_4_2 = create_dataframe_X('TrainingData/subject_004_02__x.csv', 'TrainingData/subject_004_02__x_time.csv')\n",
    "df_y_4_2 = create_dataframe_Y('TrainingData/subject_004_02__y.csv', 'TrainingData/subject_004_02__y_time.csv')\n",
    "frame_4_2 = combine_frames(df_x_4_2, df_y_4_2)\n",
    "\n",
    "# Subject_005_01\n",
    "df_x_5_1 = create_dataframe_X('TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_01__x_time.csv')\n",
    "df_y_5_1 = create_dataframe_Y('TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_01__y_time.csv')\n",
    "frame_5_1 = combine_frames(df_x_5_1, df_y_5_1)\n",
    "\n",
    "# Subject_005_02\n",
    "df_x_5_2 = create_dataframe_X('TrainingData/subject_005_02__x.csv', 'TrainingData/subject_005_02__x_time.csv')\n",
    "df_y_5_2 = create_dataframe_Y('TrainingData/subject_005_02__y.csv', 'TrainingData/subject_005_02__y_time.csv')\n",
    "frame_5_2 = combine_frames(df_x_5_2, df_y_5_2)\n",
    "\n",
    "# Subject_005_03\n",
    "df_x_5_3 = create_dataframe_X('TrainingData/subject_005_03__x.csv', 'TrainingData/subject_005_03__x_time.csv')\n",
    "df_y_5_3 = create_dataframe_Y('TrainingData/subject_005_03__y.csv', 'TrainingData/subject_005_03__y_time.csv')\n",
    "frame_5_3 = combine_frames(df_x_5_3, df_y_5_3)\n",
    "\n",
    "# Subject_006_01\n",
    "df_x_6_1 = create_dataframe_X('TrainingData/subject_006_01__x.csv', 'TrainingData/subject_006_01__x_time.csv')\n",
    "df_y_6_1 = create_dataframe_Y('TrainingData/subject_006_01__y.csv', 'TrainingData/subject_006_01__y_time.csv')\n",
    "frame_6_1 = combine_frames(df_x_6_1, df_y_6_1)\n",
    "\n",
    "# Subject_006_02\n",
    "df_x_6_2 = create_dataframe_X('TrainingData/subject_006_02__x.csv', 'TrainingData/subject_006_02__x_time.csv')\n",
    "df_y_6_2 = create_dataframe_Y('TrainingData/subject_006_02__y.csv', 'TrainingData/subject_006_02__y_time.csv')\n",
    "frame_6_2 = combine_frames(df_x_6_2, df_y_6_2)\n",
    "\n",
    "# Subject_006_03\n",
    "df_x_6_3 = create_dataframe_X('TrainingData/subject_006_03__x.csv', 'TrainingData/subject_006_03__x_time.csv')\n",
    "df_y_6_3 = create_dataframe_Y('TrainingData/subject_006_03__y.csv', 'TrainingData/subject_006_03__y_time.csv')\n",
    "frame_6_3 = combine_frames(df_x_6_3, df_y_6_3)\n",
    "\n",
    "# Subject_007_01\n",
    "df_x_7_1 = create_dataframe_X('TrainingData/subject_007_01__x.csv', 'TrainingData/subject_007_01__x_time.csv')\n",
    "df_y_7_1 = create_dataframe_Y('TrainingData/subject_007_01__y.csv', 'TrainingData/subject_007_01__y_time.csv')\n",
    "frame_7_1 = combine_frames(df_x_7_1, df_y_7_1)\n",
    "\n",
    "# Subject_007_02\n",
    "df_x_7_2 = create_dataframe_X('TrainingData/subject_007_02__x.csv', 'TrainingData/subject_007_02__x_time.csv')\n",
    "df_y_7_2 = create_dataframe_Y('TrainingData/subject_007_02__y.csv', 'TrainingData/subject_007_02__y_time.csv')\n",
    "frame_7_2 = combine_frames(df_x_7_2, df_y_7_2)\n",
    "\n",
    "# Subject_007_03\n",
    "df_x_7_3 = create_dataframe_X('TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_03__x_time.csv')\n",
    "df_y_7_3 = create_dataframe_Y('TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_03__y_time.csv')\n",
    "frame_7_3 = combine_frames(df_x_7_3, df_y_7_3)\n",
    "\n",
    "# Subject_007_04\n",
    "df_x_7_4 = create_dataframe_X('TrainingData/subject_007_04__x.csv', 'TrainingData/subject_007_04__x_time.csv')\n",
    "df_y_7_4 = create_dataframe_Y('TrainingData/subject_007_04__y.csv', 'TrainingData/subject_007_04__y_time.csv')\n",
    "frame_7_4 = combine_frames(df_x_7_4, df_y_7_4)\n",
    "\n",
    "# Subject_008_01\n",
    "df_x_8_1 = create_dataframe_X('TrainingData/subject_008_01__x.csv', 'TrainingData/subject_008_01__x_time.csv')\n",
    "df_y_8_1 = create_dataframe_Y('TrainingData/subject_008_01__y.csv', 'TrainingData/subject_008_01__y_time.csv')\n",
    "frame_8_1 = combine_frames(df_x_8_1, df_y_8_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legislative-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Combining all data frames.\n",
    "\"\"\"\n",
    "frame_list = [frame_1_1, frame_1_2, frame_1_3, frame_1_4, frame_1_5, frame_1_6, frame_1_7, frame_1_8,\n",
    "             frame_2_1, frame_2_2, frame_2_3, frame_2_4, frame_2_5,\n",
    "             frame_3_1, frame_3_2, frame_3_3,\n",
    "             frame_4_1, frame_4_2,\n",
    "             frame_5_1, frame_5_2, frame_5_3,\n",
    "             frame_6_1, frame_6_2, frame_6_3,\n",
    "             frame_7_1, frame_7_2, frame_7_3, frame_7_4,\n",
    "             frame_8_1]\n",
    "data = pd.concat(frame_list)\n",
    "\n",
    "# Create X and y\n",
    "X = data[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr', 'Time stamp']]\n",
    "y = data['Label']\n",
    "\n",
    "# Performing random undersampling on the data\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "data_resampled = pd.concat([X_resampled, y_resampled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "civic-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\"\"\"\n",
    "    Creating training and validation sets from the dataframe.\n",
    "    \n",
    "    @param frame: dataframe passed in\n",
    "    @return training and validation sets created from frame passed in\n",
    "\"\"\"\n",
    "def training_validation_split(frame):\n",
    "    frame_copy = frame.copy()\n",
    "    training_set = frame_copy.sample(frac = 0.70, random_state = 0)\n",
    "    validation_set = frame_copy.drop(training_set.index)\n",
    "    return training_set, validation_set\n",
    "\n",
    "training, val = training_validation_split(data_resampled)\n",
    "training_X = training[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr', 'Time stamp']]\n",
    "training_X = np.expand_dims(training_X, axis = 1)\n",
    "training_y = training['Label']\n",
    "training_y_encoded = to_categorical(training_y) # One-hot encoding\n",
    "val_X = val[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr', 'Time stamp']]\n",
    "val_X = np.expand_dims(val_X, axis = 1)\n",
    "val_y = val['Label']\n",
    "val_y_encoded = to_categorical(val_y) # One-hot encoding\n",
    "\n",
    "# 40916 timesteps, 7 features, 4 outputs\n",
    "n_timesteps, n_features, n_outputs = training_X.shape[1], training_X.shape[2], training_y_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "latin-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "40916/40916 [==============================] - 6s 138us/step - loss: 1.3078 - acc: 0.3650\n",
      "Epoch 2/15\n",
      "40916/40916 [==============================] - 3s 80us/step - loss: 1.2715 - acc: 0.3974\n",
      "Epoch 3/15\n",
      "40916/40916 [==============================] - 4s 107us/step - loss: 1.2530 - acc: 0.4136\n",
      "Epoch 4/15\n",
      "40916/40916 [==============================] - 3s 77us/step - loss: 1.2388 - acc: 0.4204\n",
      "Epoch 5/15\n",
      "40916/40916 [==============================] - 3s 82us/step - loss: 1.2278 - acc: 0.4252\n",
      "Epoch 6/15\n",
      "40916/40916 [==============================] - 4s 90us/step - loss: 1.2218 - acc: 0.4297\n",
      "Epoch 7/15\n",
      "40916/40916 [==============================] - 3s 82us/step - loss: 1.2111 - acc: 0.4365\n",
      "Epoch 8/15\n",
      "40916/40916 [==============================] - 3s 77us/step - loss: 1.2098 - acc: 0.4359\n",
      "Epoch 9/15\n",
      "40916/40916 [==============================] - 4s 105us/step - loss: 1.2091 - acc: 0.4317\n",
      "Epoch 10/15\n",
      "40916/40916 [==============================] - 7s 179us/step - loss: 1.2025 - acc: 0.4418\n",
      "Epoch 11/15\n",
      "40916/40916 [==============================] - 5s 118us/step - loss: 1.1962 - acc: 0.4450\n",
      "Epoch 12/15\n",
      "40916/40916 [==============================] - 6s 147us/step - loss: 1.1888 - acc: 0.4509 0s - loss: 1.1887 - acc: 0.4\n",
      "Epoch 13/15\n",
      "40916/40916 [==============================] - 6s 135us/step - loss: 1.1850 - acc: 0.4511\n",
      "Epoch 14/15\n",
      "40916/40916 [==============================] - 7s 166us/step - loss: 1.1849 - acc: 0.4491 1s - los\n",
      "Epoch 15/15\n",
      "40916/40916 [==============================] - 7s 183us/step - loss: 1.1764 - acc: 0.4564\n",
      "17536/17536 [==============================] - 1s 63us/step\n",
      "0.4911610401459854\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "from tensorflow.python.keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def define_LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape = (n_timesteps, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(n_outputs, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "def evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded):\n",
    "    verbose, epochs, batch_size = 1, 15, 64\n",
    "    model = define_LSTM_model()\n",
    "    # Fit network\n",
    "    model.fit(training_X, training_y_encoded, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "    # Evaluate model\n",
    "    _, accuracy = model.evaluate(val_X, val_y_encoded, batch_size = batch_size, verbose = verbose)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-optimum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
