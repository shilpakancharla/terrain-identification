{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confident-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.9\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 880, got 864\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "from tensorflow.python.keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import to_categorical\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-purple",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "anonymous-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Based on the training data given, we are able to extract 7 attributes:\n",
    "    1. x accelerometer measurement\n",
    "    2. y accelerometer measurement\n",
    "    3. z accelerometer measurement\n",
    "    4. x gyroscope measurement\n",
    "    5. y gyroscope measurement\n",
    "    6. z gyroscope measurement\n",
    "    7. time stamp for accelerometer and gyroscope measures\n",
    "    \n",
    "    We start by creating a dataframe using the csv files provided for readability.\n",
    "    \n",
    "    @param x_file: contains the xyz accelerometers and xyz gyroscope measures from the lower limb\n",
    "    @param x_time_file: contain the time stamps for the accelerometer and gyroscope measures\n",
    "    @return dataframe of 7 attributes mentioned\n",
    "\"\"\"\n",
    "def create_dataframe_X(x_file, x_time_file):\n",
    "    df1 = pd.read_csv(x_file, sep = ',', names = ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df1 = scale_data(df1, ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    df2 = pd.read_csv(x_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "    Scale the values of X to make it robust to outliers.\n",
    "    \n",
    "    @param df: input dataframe\n",
    "    @param columns: columns to scale\n",
    "    @return scaled dataframe\n",
    "\"\"\"\n",
    "def scale_data(df, columns):\n",
    "    scaler = RobustScaler()\n",
    "    scaler = scaler.fit(df[columns])\n",
    "    df.loc[:, columns] = scaler.transform(df[columns])\n",
    "    return df\n",
    "    \n",
    "\"\"\"\n",
    "    We have both the labels and the time stamps for the labels. We create a dataframe from these for\n",
    "    readability.\n",
    "    \n",
    "    @param y_file: contain the labels: \n",
    "        (0) indicates standing or walking in solid ground, \n",
    "        (1) indicates going down the stairs, \n",
    "        (2) indicates going up the stairs, and \n",
    "        (3) indicates walking on grass\n",
    "    @param y_time_file: contain the time stamps for the labels\n",
    "    @return dataframe of labels and time stamps\n",
    "\"\"\" \n",
    "def create_dataframe_Y(y_file, y_time_file):\n",
    "    df1 = pd.read_csv(y_file, names = ['Label'])\n",
    "    df2 = pd.read_csv(y_time_file, names = ['Time stamp'])\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames, axis = 1)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    We take the outputs of create_dataframe_X and create_dataframe_Y. In order to combine both of these\n",
    "    dataframes, we need look at the time intervals present for when the labels were assigned. We down-sample\n",
    "    the X to the shape of the y.\n",
    "    \n",
    "    @param x_frame: dataframe from create_dataframe_X\n",
    "    @param y_frame: dataframe from create_dataframe_Y\n",
    "    @return dataframe with 9 columns (8 attributes and 1 label)\n",
    "\"\"\"\n",
    "def combine_frames(x_frame, y_frame):\n",
    "    # Change each dataframe column to a list for iterations\n",
    "    time_stamp_y = y_frame['Time stamp'].tolist()\n",
    "    time_stamp_x = x_frame['Time stamp'].tolist()\n",
    "    \n",
    "    x_range = [] # Empty list to append data points to\n",
    "    x_random_row = 0 # Initializing variable to hold randomly selected row instance\n",
    "    refs = []\n",
    "    count = 0\n",
    "    for i in range(0, len(time_stamp_y)):\n",
    "        while (time_stamp_x[count] <= time_stamp_y[i]) and (count <= len(time_stamp_x)):\n",
    "            x_range.append(time_stamp_x.index(time_stamp_x[count]))\n",
    "            count += 1\n",
    "        x_random_row = random.choice(x_range) # Pick a random value\n",
    "        refs.append(x_random_row) # Keep record of selected rows\n",
    "        x_range.clear() # Clear the cache\n",
    "        continue\n",
    "    \n",
    "    # Create a new dataframe based on the refs collected - should be roughly the same length as the y_frame\n",
    "    entries = []\n",
    "    for item in refs:\n",
    "        entry = x_frame.iloc[item]\n",
    "        entries.append(entry)\n",
    "    \n",
    "    found_df = pd.concat(entries, axis = 1)\n",
    "    found_df = found_df.transpose()\n",
    "    \n",
    "    # Combine found_df with y_frame for downsampling\n",
    "    found_df = found_df.reset_index()\n",
    "    found_df = found_df.drop(['index'], axis = 1)\n",
    "    found_df = found_df.drop(['Time stamp'], axis = 1)\n",
    "    combined_frame = pd.concat([found_df, y_frame], axis = 1)\n",
    "    return combined_frame\n",
    "\n",
    "\"\"\"\n",
    "    Takes in the sequential X and y and creates windows of time-series data.\n",
    "    \n",
    "    @param X: input data\n",
    "    @param y: label data\n",
    "    @param time_steps: determines size of window\n",
    "    @param step: incremental value that window will slide over\n",
    "    @return time series of X and y data\n",
    "\"\"\"\n",
    "def mode_labels(X, y, time_steps, step):\n",
    "    X_values = []\n",
    "    y_values = []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        value = X.iloc[i:(i + time_steps)].values\n",
    "        labels = y.iloc[i: (i + time_steps)]\n",
    "        X_values.append(value)\n",
    "        y_values.append(stats.mode(labels)[0][0])\n",
    "    return np.array(X_values), np.array(y_values).reshape(-1, 1)\n",
    "\n",
    "\"\"\"\n",
    "    Generating data frames from training data.\n",
    "    \n",
    "    @param X_file: list of input X files\n",
    "    @param X_t_file: list of input X_time files\n",
    "    @param y_file: list of input y files\n",
    "    @param y_t file: list of y_time files\n",
    "    @return stacked window of instances across all training files, stack window of labels across all label files\n",
    "\"\"\"\n",
    "def generate_data(X_file, X_t_file, y_file, y_t_file):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    for item_X, item_X_t, item_y, item_y_t in zip(X_file, X_t_file, y_file, y_t_file):\n",
    "        df_x = create_dataframe_X(item_X, item_X_t)\n",
    "        df_y = create_dataframe_Y(item_y, item_y_t)\n",
    "        combined_frame = combine_frames(df_x, df_y)\n",
    "        X_temp = combined_frame[['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr']]\n",
    "        y_temp = combined_frame['Label']\n",
    "        X, y = mode_labels(X_temp, y_temp, 30, 1)\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    return np.concatenate(all_X), np.concatenate(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposed-moment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301655, 30, 6) (301655, 1)\n",
      "(33762, 30, 6) (33762, 1)\n"
     ]
    }
   ],
   "source": [
    "# List of training X_files\n",
    "X_files = ['TrainingData/subject_001_01__x.csv', 'TrainingData/subject_001_02__x.csv', \n",
    "           'TrainingData/subject_001_03__x.csv', 'TrainingData/subject_001_04__x.csv',\n",
    "           'TrainingData/subject_001_05__x.csv', 'TrainingData/subject_001_06__x.csv',\n",
    "           'TrainingData/subject_001_07__x.csv', 'TrainingData/subject_001_08__x.csv',\n",
    "           'TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_02__x.csv',\n",
    "           'TrainingData/subject_002_03__x.csv', 'TrainingData/subject_001_04__x.csv',\n",
    "           'TrainingData/subject_002_05__x.csv', 'TrainingData/subject_003_01__x.csv',\n",
    "           'TrainingData/subject_005_01__x.csv', 'TrainingData/subject_005_02__x.csv',\n",
    "           'TrainingData/subject_005_03__x.csv', 'TrainingData/subject_006_01__x.csv',\n",
    "           'TrainingData/subject_006_02__x.csv', 'TrainingData/subject_006_03__x.csv',\n",
    "           'TrainingData/subject_007_01__x.csv', 'TrainingData/subject_007_02__x.csv',\n",
    "           'TrainingData/subject_007_03__x.csv', 'TrainingData/subject_007_04__x.csv',\n",
    "           'TrainingData/subject_008_01__x.csv']\n",
    "\n",
    "# List of training X_t_files\n",
    "X_t_files = ['TrainingData/subject_001_01__x_time.csv', 'TrainingData/subject_001_02__x_time.csv', \n",
    "             'TrainingData/subject_001_03__x_time.csv', 'TrainingData/subject_001_04__x_time.csv',\n",
    "             'TrainingData/subject_001_05__x_time.csv', 'TrainingData/subject_001_06__x_time.csv',\n",
    "             'TrainingData/subject_001_07__x_time.csv', 'TrainingData/subject_001_08__x_time.csv',\n",
    "             'TrainingData/subject_002_01__x_time.csv', 'TrainingData/subject_002_02__x_time.csv',\n",
    "             'TrainingData/subject_002_03__x_time.csv', 'TrainingData/subject_001_04__x_time.csv',\n",
    "             'TrainingData/subject_002_05__x_time.csv', 'TrainingData/subject_003_01__x_time.csv',\n",
    "             'TrainingData/subject_005_01__x_time.csv', 'TrainingData/subject_005_02__x_time.csv',\n",
    "             'TrainingData/subject_005_03__x_time.csv', 'TrainingData/subject_006_01__x_time.csv',\n",
    "             'TrainingData/subject_006_02__x_time.csv', 'TrainingData/subject_006_03__x_time.csv',\n",
    "             'TrainingData/subject_007_01__x_time.csv', 'TrainingData/subject_007_02__x_time.csv',\n",
    "             'TrainingData/subject_007_03__x_time.csv', 'TrainingData/subject_007_04__x_time.csv',\n",
    "             'TrainingData/subject_008_01__x_time.csv']\n",
    "\n",
    "# List of training y_files\n",
    "y_files = ['TrainingData/subject_001_01__y.csv', 'TrainingData/subject_001_02__y.csv', \n",
    "           'TrainingData/subject_001_03__y.csv', 'TrainingData/subject_001_04__y.csv',\n",
    "           'TrainingData/subject_001_05__y.csv', 'TrainingData/subject_001_06__y.csv',\n",
    "           'TrainingData/subject_001_07__y.csv', 'TrainingData/subject_001_08__y.csv',\n",
    "           'TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_02__y.csv',\n",
    "           'TrainingData/subject_002_03__y.csv', 'TrainingData/subject_001_04__y.csv',\n",
    "           'TrainingData/subject_002_05__y.csv', 'TrainingData/subject_003_01__y.csv',\n",
    "           'TrainingData/subject_005_01__y.csv', 'TrainingData/subject_005_02__y.csv',\n",
    "           'TrainingData/subject_005_03__y.csv', 'TrainingData/subject_006_01__y.csv',\n",
    "           'TrainingData/subject_006_02__y.csv', 'TrainingData/subject_006_03__y.csv',\n",
    "           'TrainingData/subject_007_01__y.csv', 'TrainingData/subject_007_02__y.csv',\n",
    "           'TrainingData/subject_007_03__y.csv', 'TrainingData/subject_007_04__y.csv',\n",
    "           'TrainingData/subject_008_01__y.csv']\n",
    "\n",
    "# List of training y_t_files\n",
    "y_t_files = ['TrainingData/subject_001_01__y_time.csv', 'TrainingData/subject_001_02__y_time.csv', \n",
    "             'TrainingData/subject_001_03__y_time.csv', 'TrainingData/subject_001_04__y_time.csv',\n",
    "             'TrainingData/subject_001_05__y_time.csv', 'TrainingData/subject_001_06__y_time.csv',\n",
    "             'TrainingData/subject_001_07__y_time.csv', 'TrainingData/subject_001_08__y_time.csv',\n",
    "             'TrainingData/subject_002_01__y_time.csv', 'TrainingData/subject_002_02__y_time.csv',\n",
    "             'TrainingData/subject_002_03__y_time.csv', 'TrainingData/subject_001_04__y_time.csv',\n",
    "             'TrainingData/subject_002_05__y_time.csv', 'TrainingData/subject_003_01__y_time.csv',\n",
    "             'TrainingData/subject_005_01__y_time.csv', 'TrainingData/subject_005_02__y_time.csv',\n",
    "             'TrainingData/subject_005_03__y_time.csv', 'TrainingData/subject_006_01__y_time.csv',\n",
    "             'TrainingData/subject_006_02__y_time.csv', 'TrainingData/subject_006_03__y_time.csv',\n",
    "             'TrainingData/subject_007_01__y_time.csv', 'TrainingData/subject_007_02__y_time.csv',\n",
    "             'TrainingData/subject_007_03__y_time.csv', 'TrainingData/subject_007_04__y_time.csv',\n",
    "             'TrainingData/subject_008_01__y_time.csv']\n",
    "\n",
    "# Use some files to create validation set\n",
    "val_X = ['TrainingData/subject_003_02__x.csv', 'TrainingData/subject_003_03__x.csv',\n",
    "         'TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_02__x.csv',]\n",
    "val_X_t = ['TrainingData/subject_003_02__x_time.csv', 'TrainingData/subject_003_03__x_time.csv',\n",
    "           'TrainingData/subject_004_01__x_time.csv', 'TrainingData/subject_004_02__x_time.csv',]\n",
    "val_y = ['TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_03__y.csv',\n",
    "         'TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_02__y.csv',]\n",
    "val_y_t = ['TrainingData/subject_003_02__y_time.csv', 'TrainingData/subject_003_03__y_time.csv',\n",
    "           'TrainingData/subject_004_01__y_time.csv', 'TrainingData/subject_004_02__y_time.csv',]\n",
    "\n",
    "# TODO: Create the test set\n",
    "#test_X = ['TestData/subject_009_01__x.csv', 'TestData/subject_010_01__x.csv',\n",
    "#          'TestData/subject_011_01__x.csv', 'TestData/subject_012_01__x.csv']\n",
    "#test_X_t = ['TestData/subject_009_01__x_time.csv', 'TestData/subject_010_01__x_time.csv',\n",
    "#           'TestData/subject_011_01__x_time.csv', 'TestData/subject_012_01__x_time.csv']\n",
    "\n",
    "training_X, training_y = generate_data(X_files, X_t_files, y_files, y_t_files)\n",
    "val_X, val_y = generate_data(val_X, val_X_t, val_y, val_y_t)\n",
    "print(training_X.shape, training_y.shape)\n",
    "print(val_X.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legislative-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33593218 5.81133929 4.44263623 1.59734284]\n",
      "{0: 0.3359321754546953, 1: 5.811339292594591, 2: 4.442636229749632, 3: 1.5973428365669744}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "label_weights = class_weight.compute_class_weight('balanced', np.unique(training_y), training_y.ravel())\n",
    "print(label_weights)\n",
    "label_weights = {i:label_weights[i] for i in range(len(label_weights))} # Create dictionary\n",
    "print(label_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aboriginal-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "encoder = encoder.fit(training_y)\n",
    "training_y_encoded = encoder.transform(training_y)\n",
    "val_y_encoded = encoder.transform(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acquired-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 301655 samples, validate on 33762 samples\n",
      "Epoch 1/15\n",
      "301655/301655 [==============================] - 250s 830us/step - loss: 0.3490 - acc: 0.7632 - val_loss: 0.3825 - val_acc: 0.8343\n",
      "Epoch 2/15\n",
      "301655/301655 [==============================] - 188s 622us/step - loss: 0.1871 - acc: 0.8710 - val_loss: 0.4841 - val_acc: 0.7920\n",
      "Epoch 3/15\n",
      "301655/301655 [==============================] - 394s 1ms/step - loss: 0.1343 - acc: 0.9044 - val_loss: 0.5184 - val_acc: 0.7958\n",
      "Epoch 4/15\n",
      "301655/301655 [==============================] - 180s 598us/step - loss: 0.1022 - acc: 0.9263 - val_loss: 0.3183 - val_acc: 0.8715\n",
      "Epoch 5/15\n",
      "301655/301655 [==============================] - 184s 610us/step - loss: 0.0825 - acc: 0.9417 - val_loss: 0.6074 - val_acc: 0.8100\n",
      "Epoch 6/15\n",
      "301655/301655 [==============================] - 219s 727us/step - loss: 0.0690 - acc: 0.9517 - val_loss: 0.4451 - val_acc: 0.8388\n",
      "Epoch 7/15\n",
      "301655/301655 [==============================] - 175s 581us/step - loss: 0.0555 - acc: 0.9612 - val_loss: 0.3928 - val_acc: 0.8830\n",
      "Epoch 8/15\n",
      "301655/301655 [==============================] - 182s 602us/step - loss: 0.0512 - acc: 0.9653 - val_loss: 0.4603 - val_acc: 0.8576\n",
      "Epoch 9/15\n",
      "301655/301655 [==============================] - 249s 824us/step - loss: 0.0417 - acc: 0.9714 - val_loss: 0.4006 - val_acc: 0.8745\n",
      "Epoch 10/15\n",
      "301655/301655 [==============================] - 279s 926us/step - loss: 0.0398 - acc: 0.9735 - val_loss: 0.3573 - val_acc: 0.8994\n",
      "Epoch 11/15\n",
      "301655/301655 [==============================] - 208s 690us/step - loss: 0.0354 - acc: 0.9760 - val_loss: 0.3692 - val_acc: 0.9104\n",
      "Epoch 12/15\n",
      "301655/301655 [==============================] - 251s 833us/step - loss: 0.0339 - acc: 0.9776 - val_loss: 0.3297 - val_acc: 0.9082\n",
      "Epoch 13/15\n",
      "301655/301655 [==============================] - 226s 748us/step - loss: 0.0319 - acc: 0.9793 - val_loss: 0.4114 - val_acc: 0.8933\n",
      "Epoch 14/15\n",
      "301655/301655 [==============================] - 201s 665us/step - loss: 0.0286 - acc: 0.9814 - val_loss: 0.5100 - val_acc: 0.8854\n",
      "Epoch 15/15\n",
      "301655/301655 [==============================] - 208s 691us/step - loss: 0.0286 - acc: 0.9816 - val_loss: 0.5014 - val_acc: 0.8801\n",
      "33762/33762 [==============================] - 6s 173us/step\n",
      "0.8801315087968722\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features, n_outputs = training_X.shape[1], training_X.shape[2], training_y_encoded.shape[1]\n",
    "\n",
    "def define_LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = 100, input_shape = (n_timesteps, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 100, activation = 'relu'))\n",
    "    model.add(Dense(units = n_outputs, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "def evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded):\n",
    "    verbose, epochs, batch_size = 1, 15, 64\n",
    "    model = define_LSTM_model()\n",
    "    # Fit network\n",
    "    model.fit(training_X, training_y_encoded, epochs = epochs, batch_size = batch_size, \n",
    "              validation_data = (val_X, val_y_encoded), class_weight = label_weights, verbose = verbose)\n",
    "    # Evaluate model\n",
    "    _, accuracy = model.evaluate(val_X, val_y_encoded, batch_size = batch_size, verbose = verbose)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
