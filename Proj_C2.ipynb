{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confident-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import statistics as st\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install keras-rectified-adam\n",
    "\n",
    "# Import custom functions\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of training X_files\n",
    "X_files = ['TrainingData/subject_001_02__x.csv', 'TrainingData/subject_001_03__x.csv', \n",
    "           'TrainingData/subject_001_04__x.csv', 'TrainingData/subject_001_05__x.csv', \n",
    "           'TrainingData/subject_001_06__x.csv', 'TrainingData/subject_001_07__x.csv', \n",
    "           'TrainingData/subject_002_01__x.csv', 'TrainingData/subject_002_03__x.csv', \n",
    "           'TrainingData/subject_002_04__x.csv', 'TrainingData/subject_002_05__x.csv', \n",
    "           'TrainingData/subject_003_01__x.csv', 'TrainingData/subject_005_02__x.csv',\n",
    "           'TrainingData/subject_005_03__x.csv', 'TrainingData/subject_006_01__x.csv', \n",
    "           'TrainingData/subject_006_03__x.csv', 'TrainingData/subject_007_01__x.csv', \n",
    "           'TrainingData/subject_007_02__x.csv', 'TrainingData/subject_007_03__x.csv',\n",
    "           'TrainingData/subject_001_01__x.csv', 'TrainingData/subject_003_02__x.csv', \n",
    "           'TrainingData/subject_003_03__x.csv', 'TrainingData/subject_005_01__x.csv',\n",
    "           'TrainingData/subject_004_01__x.csv', 'TrainingData/subject_004_02__x.csv',\n",
    "           'TrainingData/subject_008_01__x.csv', 'TrainingData/subject_007_04__x.csv',\n",
    "           'TrainingData/subject_002_02__x.csv', 'TrainingData/subject_006_02__x.csv',\n",
    "           'TrainingData/subject_001_08__x.csv']\n",
    "\n",
    "# List of training X_t_files\n",
    "X_t_files = ['TrainingData/subject_001_02__x_time.csv', 'TrainingData/subject_001_03__x_time.csv', \n",
    "             'TrainingData/subject_001_04__x_time.csv', 'TrainingData/subject_001_05__x_time.csv', \n",
    "             'TrainingData/subject_001_06__x_time.csv', 'TrainingData/subject_001_07__x_time.csv', \n",
    "             'TrainingData/subject_002_01__x_time.csv', 'TrainingData/subject_002_03__x_time.csv', \n",
    "             'TrainingData/subject_002_04__x_time.csv', 'TrainingData/subject_002_05__x_time.csv', \n",
    "             'TrainingData/subject_003_01__x_time.csv', 'TrainingData/subject_005_02__x_time.csv', \n",
    "             'TrainingData/subject_005_03__x_time.csv', 'TrainingData/subject_006_01__x_time.csv',  \n",
    "             'TrainingData/subject_006_03__x_time.csv', 'TrainingData/subject_007_01__x_time.csv', \n",
    "             'TrainingData/subject_007_02__x_time.csv', 'TrainingData/subject_007_03__x_time.csv',\n",
    "             'TrainingData/subject_001_01__x_time.csv', 'TrainingData/subject_005_01__x_time.csv',\n",
    "             'TrainingData/subject_003_02__x_time.csv', 'TrainingData/subject_003_03__x_time.csv',\n",
    "             'TrainingData/subject_004_01__x_time.csv', 'TrainingData/subject_004_02__x_time.csv',\n",
    "             'TrainingData/subject_008_01__x_time.csv', 'TrainingData/subject_007_04__x_time.csv',\n",
    "             'TrainingData/subject_002_02__x_time.csv', 'TrainingData/subject_006_02__x_time.csv',\n",
    "             'TrainingData/subject_001_08__x_time.csv']\n",
    "\n",
    "# List of training y_files\n",
    "y_files = ['TrainingData/subject_001_02__y.csv', 'TrainingData/subject_001_03__y.csv',\n",
    "           'TrainingData/subject_001_04__y.csv', 'TrainingData/subject_001_05__y.csv',\n",
    "           'TrainingData/subject_001_06__y.csv', 'TrainingData/subject_001_07__y.csv', \n",
    "           'TrainingData/subject_002_01__y.csv', 'TrainingData/subject_002_03__y.csv', \n",
    "           'TrainingData/subject_002_04__y.csv', 'TrainingData/subject_002_05__y.csv', \n",
    "           'TrainingData/subject_003_01__y.csv', 'TrainingData/subject_005_02__y.csv', \n",
    "           'TrainingData/subject_005_03__y.csv', 'TrainingData/subject_006_01__y.csv',  \n",
    "           'TrainingData/subject_006_03__y.csv', 'TrainingData/subject_007_01__y.csv', \n",
    "           'TrainingData/subject_007_02__y.csv', 'TrainingData/subject_007_03__y.csv',\n",
    "           'TrainingData/subject_001_01__y.csv', 'TrainingData/subject_005_01__y.csv',\n",
    "           'TrainingData/subject_003_02__y.csv', 'TrainingData/subject_003_03__y.csv',\n",
    "           'TrainingData/subject_004_01__y.csv', 'TrainingData/subject_004_02__y.csv',\n",
    "           'TrainingData/subject_008_01__y.csv', 'TrainingData/subject_007_04__y.csv',\n",
    "           'TrainingData/subject_002_02__y.csv', 'TrainingData/subject_006_02__y.csv',\n",
    "           'TrainingData/subject_001_08__y.csv']\n",
    "\n",
    "y_t_files = ['TrainingData/subject_001_02__y_time.csv', 'TrainingData/subject_001_03__y_time.csv', \n",
    "             'TrainingData/subject_001_04__y_time.csv', 'TrainingData/subject_001_05__y_time.csv', \n",
    "             'TrainingData/subject_001_06__y_time.csv', 'TrainingData/subject_001_07__y_time.csv', \n",
    "             'TrainingData/subject_002_01__y_time.csv', 'TrainingData/subject_002_03__y_time.csv', \n",
    "             'TrainingData/subject_002_04__y_time.csv', 'TrainingData/subject_002_05__y_time.csv', \n",
    "             'TrainingData/subject_003_01__y_time.csv', 'TrainingData/subject_005_02__y_time.csv', \n",
    "             'TrainingData/subject_005_03__y_time.csv', 'TrainingData/subject_006_01__y_time.csv',  \n",
    "             'TrainingData/subject_006_03__y_time.csv', 'TrainingData/subject_007_01__y_time.csv', \n",
    "             'TrainingData/subject_007_02__y_time.csv', 'TrainingData/subject_007_03__y_time.csv',\n",
    "             'TrainingData/subject_001_01__y_time.csv', 'TrainingData/subject_005_01__y_time.csv',\n",
    "             'TrainingData/subject_003_02__y_time.csv', 'TrainingData/subject_003_03__y_time.csv',\n",
    "             'TrainingData/subject_004_01__y_time.csv', 'TrainingData/subject_004_02__y_time.csv',\n",
    "             'TrainingData/subject_008_01__y_time.csv', 'TrainingData/subject_007_04__y_time.csv',\n",
    "             'TrainingData/subject_002_02__y_time.csv', 'TrainingData/subject_006_02__y_time.csv',\n",
    "             'TrainingData/subject_001_08__y_time.csv']\n",
    "\n",
    "training_X, training_y = generate_data(X_files, X_t_files, y_files, y_t_files)\n",
    "print(training_X.shape, training_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data to .npy files so we do not have to generate them repeatedly\n",
    "np.save('training_X.npy', training_X)\n",
    "np.save('training_y.npy', training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "british-usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334543, 30, 6) (334543, 1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data back\n",
    "training_X = np.load('training_X.npy')\n",
    "training_y = np.load('training_y.npy')\n",
    "print(training_X.shape, training_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class weights\n",
    "label_weights = get_label_weights(training_X, training_y)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "training_y_encoded = one_hot_encoding(training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------\n",
      "Training for fold 1...\n",
      "Epoch 1/25\n",
      "207488/267634 [======================>.......] - ETA: 44s - loss: nan - acc: 0.7506 - f1: nan - precision_measure: nan - recall_measure: nan"
     ]
    }
   ],
   "source": [
    "# Define the k-fold cross validator\n",
    "k_fold = KFold(n_splits = 5, shuffle = True)\n",
    "cv_scores_accuracy = []\n",
    "cv_scores_loss = []\n",
    "scores = [] # Holds the metrics\n",
    "history = None # Placeholder for history\n",
    "# K-fold cross validation  model evaluation\n",
    "fold_no = 1\n",
    "for train, test in k_fold.split(training_X, training_y_encoded):\n",
    "    # From previous runs, we have found that dropout_rate = 0.1, l1 = 2**-6, and l2 = 2**-8 gave optimal results.\n",
    "    model = define_LSTM_model(training_X, training_y_encoded, dropout_rate = 0.1, l1_value = 2**-6, l2_value = 2**-8)\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no}...')\n",
    "    # Fit the model\n",
    "    history = model.fit(training_X[train], training_y_encoded[train], epochs = 25, batch_size = 64, \n",
    "                        class_weight = label_weights, verbose = 1)\n",
    "    # Evaluate the model, generate metrics\n",
    "    scores = model.evaluate(training_X[test], training_y_encoded[test], batch_size = 64, verbose = 0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {score[0]}; {model.metrics_names[1]} of {score[1]*100}%; {model.metrics_names[2]} of {score[2]}; {model.metrics_names[3]} of {score[3]}; {model.metrics_names[4]} of {score[4]}')\n",
    "    cv_scores_accuracy.append(scores[1] * 100)\n",
    "    cv_scores_loss.append(scores[0])\n",
    "    # Increment the fold number\n",
    "    fold_no += 1\n",
    "  \n",
    "\"\"\"\n",
    "def evaluate_model(training_X, training_y_encoded, val_X, val_y_encoded, dropout_rate):\n",
    "    verbose, epochs, batch_size = 1, 50, 64\n",
    "    model = define_LSTM_model(dropout_rate)\n",
    "    model.summary()\n",
    "    # Fit network\n",
    "    history = model.fit(training_X, training_y_encoded, epochs = epochs, batch_size = batch_size, \n",
    "              validation_data = (val_X, val_y_encoded), class_weight = label_weights, verbose = verbose)\n",
    "    # Evaluate model\n",
    "    loss, accuracy, f1, precision, recall = model.evaluate(val_X, val_y_encoded, \n",
    "                                                           batch_size = batch_size, verbose = verbose)\n",
    "    return model, history, accuracy, f1, precision, recall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal parameters: dropout_rate = 0.5, l1 = 2e-6, l2 = 2e-6\n",
    "best_model, best_history, best_accuracy, best_f1, best_precision, best_recall = evaluate_model(training_X, \n",
    "                                                                                   training_y_encoded, \n",
    "                                                                                   val_X, val_y_encoded, \n",
    "                                                                                   0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(best_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_acc = st.mean(best_history.history['val_acc'][45:50])\n",
    "print(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction report\n",
    "y_pred = best_model.predict(val_X, batch_size = 64, verbose = 1)\n",
    "y_pred_bool = np.argmax(y_pred, axis = 1)\n",
    "print(classification_report(val_y, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.DataFrame({'Y':val_y.flatten(), 'Prediction':y_pred_bool})\n",
    "validation = pd.DataFrame(list(zip(val_y.flatten(), y_pred_bool)), columns = ['Y', 'Prediction'])\n",
    "validation.to_csv('validation_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions using test data\n",
    "test_files = ['TestData/subject_009_01__x.csv', 'TestData/subject_010_01__x.csv', \n",
    "             'TestData/subject_011_01__x.csv', 'TestData/subject_012_01__x.csv']\n",
    "\n",
    "y_files = ['TestData/subject_009_01__y_time.csv', 'TestData/subject_010_01__y_time.csv',\n",
    "          'TestData/subject_011_01__y_time.csv', 'TestData/subject_012_01__y_time.csv']\n",
    "\n",
    "prediction_files = ['subject_009_01__y_prediction.csv', 'subject_010_01__y_prediction.csv',\n",
    "                   'subject_011_01__y_prediction.csv', 'subject_012_01__y_prediction.csv']\n",
    "\n",
    "def generate_test_data(X, time_steps, step):\n",
    "    X_values = []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        value = X.iloc[i:(i + time_steps)].values\n",
    "        X_values.append(value)\n",
    "    return np.array(X_values)\n",
    "\n",
    "def reduce(y):\n",
    "    y_output = []\n",
    "    for i in range(0, y.shape[0], 4):\n",
    "        item = list(y[i:i + 4])\n",
    "        y_output.append(max(item, key = item.count))\n",
    "    return np.array(y_output)\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    test_input = pd.read_csv(test_files[i])\n",
    "    test_input = scale_data(test_input, ['X_acc', 'Y_acc', 'Z_acc', 'X_gyr', 'Y_gyr', 'Z_gyr'])\n",
    "    y_frame = pd.read_csv(y_files[i])\n",
    "    print(y_frame.shape)\n",
    "\n",
    "    add_to_frame = y_frame.shape[0] * 4 - test_input.shape[0] + 30\n",
    "    adding_dataframe = pd.DataFrame(test_input.iloc[-add_to_frame:])\n",
    "    test_input = test_input.append(adding_dataframe)\n",
    "    \n",
    "    # Apply window technique\n",
    "    X_test = generate_test_data(test_input, 30, 1)\n",
    "\n",
    "    y_hat_encoded = best_model.predict(X_test, batch_size = 64, verbose = 1)\n",
    "    \n",
    "    y_hat = np.argmax(y_hat_encoded, axis = 1)\n",
    "    \n",
    "    y_actual = reduce(y_hat)\n",
    "    #y_actual = y_actual[np.arange(y_actual.size - 1)]\n",
    "    \n",
    "    print(y_actual.shape)\n",
    "    \n",
    "    y_series = pd.Series(y_actual)\n",
    "    y_series.to_csv(\"C2_predictions/\" + prediction_files[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
